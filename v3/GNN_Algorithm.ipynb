{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8370a2d5",
   "metadata": {},
   "source": [
    "### ë°ì´í„° ìˆ˜ì§‘"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "689fec1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "!uv add torch torch_geometric pandas networkx numpy python-dotenv\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "4a0ebeec",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import time\n",
    "import pandas as pd\n",
    "\n",
    "from datetime import datetime, timezone\n",
    "from dotenv import load_dotenv\n",
    "import os\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e6b2cc2",
   "metadata": {},
   "source": [
    "\n",
    "# .env íŒŒì¼ ë¡œë“œ\n",
    "load_dotenv()\n",
    "\n",
    "# --------------------------------\n",
    "# ì„¤ì •\n",
    "# --------------------------------\n",
    "API_KEY = os.getenv('ETHEREUM_API_KEY')  # Etherscan API í‚¤ ì…ë ¥\n",
    "CONTRACT = os.getenv('ETHEREUM_TOKEN_CONTRACT_ADDRESS')  # PicaArtMoney í† í° ì»¨íŠ¸ë™íŠ¸\n",
    "START_DATE = \"2020-10-13\"\n",
    "END_DATE   = \"2024-08-07\"\n",
    "\n",
    "# --------------------------\n",
    "# ìœ í‹¸: UTC íƒ€ì„ìŠ¤íƒ¬í”„ ë³€í™˜\n",
    "# --------------------------\n",
    "def to_ts_utc(dstr: str) -> int:\n",
    "    y, m, d = map(int, dstr.split(\"-\"))\n",
    "    return int(datetime(y, m, d, tzinfo=timezone.utc).timestamp())\n",
    "\n",
    "start_ts = to_ts_utc(START_DATE)\n",
    "end_ts   = to_ts_utc(END_DATE)\n",
    "\n",
    "# --------------------------\n",
    "# ë¸”ë¡ ë²ˆí˜¸ by timestamp\n",
    "# --------------------------\n",
    "def get_block_number_by_timestamp(ts: int, closest=\"before\") -> int:\n",
    "    url = \"https://api.etherscan.io/api\"\n",
    "    params = {\n",
    "        \"module\": \"block\",\n",
    "        \"action\": \"getblocknobytime\",\n",
    "        \"timestamp\": ts,\n",
    "        \"closest\": closest,\n",
    "        \"apikey\": API_KEY\n",
    "    }\n",
    "    r = requests.get(url, params=params, timeout=30).json()\n",
    "    return int(r[\"result\"])\n",
    "\n",
    "start_block = get_block_number_by_timestamp(start_ts, \"after\")\n",
    "end_block   = get_block_number_by_timestamp(end_ts, \"before\")\n",
    "print(f\"[INFO] Block range: {start_block} ~ {end_block}\")\n",
    "\n",
    "# --------------------------\n",
    "# í˜ì´ì§€ í˜¸ì¶œ (ì¬ì‹œë„ í¬í•¨)\n",
    "# --------------------------\n",
    "def fetch_transfers_page(page, offset, start_block, end_block):\n",
    "    url = \"https://api.etherscan.io/api\"\n",
    "    params = {\n",
    "        \"module\": \"account\",\n",
    "        \"action\": \"tokentx\",\n",
    "        \"contractaddress\": CONTRACT,\n",
    "        \"startblock\": start_block,\n",
    "        \"endblock\": end_block,\n",
    "        \"page\": page,\n",
    "        \"offset\": offset,\n",
    "        \"sort\": \"asc\",\n",
    "        \"apikey\": API_KEY\n",
    "    }\n",
    "    # ê°„ë‹¨ ì¬ì‹œë„\n",
    "    for i in range(5):\n",
    "        try:\n",
    "            resp = requests.get(url, params=params, timeout=30).json()\n",
    "            # status==\"1\"ì´ë©´ ê²°ê³¼, \"0\"ì´ë©´ ì—†ìŒ(ë˜ëŠ” rate limit)\n",
    "            if resp.get(\"status\") == \"1\":\n",
    "                return resp[\"result\"]\n",
    "            # \"Max rate limit reached\" ê°™ì€ ë©”ì‹œì§€ë©´ ì ê¹ ëŒ€ê¸° í›„ ì¬ì‹œë„\n",
    "            if \"Max rate limit reached\" in resp.get(\"message\",\"\"):\n",
    "                time.sleep(1.0 * (i+1))\n",
    "                continue\n",
    "            return []\n",
    "        except Exception:\n",
    "            time.sleep(1.0 * (i+1))\n",
    "    return []\n",
    "\n",
    "# --------------------------\n",
    "# ë²”ìœ„ ìˆ˜ì§‘ (ë¹ˆ ì²­í¬ë„ ë¡œê·¸)\n",
    "# --------------------------\n",
    "all_txs = []\n",
    "step = 200_000        # ë¸”ë¡ ì²­í¬ í¬ê¸°\n",
    "offset = 10_000       # í˜ì´ì§€ í¬ê¸°\n",
    "sleep_s = 0.25        # rate limit ë³´í˜¸\n",
    "current = start_block\n",
    "\n",
    "# (ì„ íƒ) ì²˜ë¦¬í•œ ì²­í¬ ëª©ë¡ ë³´ê´€í•´ ëˆ„ë½ ì‹œê°í™”\n",
    "processed_ranges = []\n",
    "\n",
    "while current <= end_block:\n",
    "    to_block = min(current + step - 1, end_block)\n",
    "    processed_ranges.append((current, to_block))\n",
    "\n",
    "    page = 1\n",
    "    new_cnt = 0\n",
    "    while True:\n",
    "        txs = fetch_transfers_page(page, offset, current, to_block)\n",
    "        if not txs:\n",
    "            # ì²« í˜ì´ì§€ë¶€í„° ë¹ˆ ê²½ìš° â†’ ì´ ì²­í¬ì—” íŠ¸ëœì­ì…˜ ì—†ìŒì„ ëª…ì‹œ\n",
    "            if page == 1:\n",
    "                print(f\"[INFO] Blocks {current}â€“{to_block}, Page {page}, NO TX\")\n",
    "            break\n",
    "\n",
    "        # ê¸°ê°„ í•„í„°\n",
    "        for tx in txs:\n",
    "            ts = int(tx.get(\"timeStamp\", 0))\n",
    "            if start_ts <= ts <= end_ts:\n",
    "                all_txs.append(tx)\n",
    "                new_cnt += 1\n",
    "\n",
    "        print(f\"[INFO] Blocks {current}â€“{to_block}, Page {page}, Added {new_cnt}, Total {len(all_txs)}\")\n",
    "        page += 1\n",
    "        time.sleep(sleep_s)\n",
    "\n",
    "        # ë§ˆì§€ë§‰ í˜ì´ì§€(=ë°˜í™˜ê±´ìˆ˜ < offset) íŒë‹¨ â†’ ë” ì´ìƒ ì´ ì²­í¬ì—ì„œ í˜ì´ì§€ ì—†ìŒ\n",
    "        if len(txs) < offset:\n",
    "            break\n",
    "\n",
    "    current = to_block + 1  # ë‹¤ìŒ ì²­í¬ë¡œ\n",
    "\n",
    "# --------------------------\n",
    "# DataFrame & ì¤‘ë³µ ì œê±°\n",
    "# --------------------------\n",
    "df = pd.DataFrame(all_txs)\n",
    "\n",
    "# ì–´ë–¤ ì»¬ëŸ¼ì´ ìˆëŠ”ì§€ í™•ì¸\n",
    "print(\"[INFO] Columns:\", df.columns.tolist())\n",
    "\n",
    "# ì¡´ì¬í•˜ëŠ” ì»¬ëŸ¼ë§Œìœ¼ë¡œ ì¤‘ë³µ ì œê±°\n",
    "subset_cols = [c for c in [\"hash\", \"logIndex\", \"transactionIndex\"] if c in df.columns]\n",
    "if subset_cols:\n",
    "    df.drop_duplicates(subset=subset_cols, inplace=True)\n",
    "else:\n",
    "    df.drop_duplicates(subset=[\"hash\"], inplace=True)\n",
    "\n",
    "# (ì„ íƒ) amount ì»¬ëŸ¼ ì¶”ê°€\n",
    "if {\"value\",\"tokenDecimal\"} <= set(df.columns):\n",
    "    df[\"amount\"] = (df[\"value\"].astype(\"int64\") /\n",
    "                    (10 ** df[\"tokenDecimal\"].astype(int)))\n",
    "else:\n",
    "    df[\"amount\"] = 0.0\n",
    "\n",
    "out_path = \"picaartmoney_transactions_full.csv\"\n",
    "df.to_csv(out_path, index=False)\n",
    "print(f\"[INFO] Finished. Total collected: {len(df)} transactions â†’ {out_path}\")\n",
    "\n",
    "# --------------------------\n",
    "# (ì„ íƒ) ëˆ„ë½ êµ¬ê°„ ì‹œê° í™•ì¸ìš©\n",
    "# --------------------------\n",
    "# ë¹ˆ ì²­í¬ë„ NO TX ë¡œê·¸ê°€ ì°íˆë¯€ë¡œ, ì²˜ë¦¬ ë²”ìœ„ê°€ ì—°ì†ì ì´ë¼ë©´ ëˆ„ë½ì€ ì—†ìŠµë‹ˆë‹¤.\n",
    "# ê·¸ë˜ë„ ì•ˆì‹¬ìš©ìœ¼ë¡œ ì—°ì†ì„± ì²´í¬:\n",
    "gaps = []\n",
    "for (a1,b1),(a2,b2) in zip(processed_ranges, processed_ranges[1:]):\n",
    "    if a2 != b1 + 1:\n",
    "        gaps.append((b1+1, a2-1))\n",
    "if gaps:\n",
    "    print(\"[WARN] Detected block gaps in iteration (should not happen):\", gaps)\n",
    "else:\n",
    "    print(\"[INFO] No block range gaps in iteration (every chunk covered).\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "943fd7de",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a7fa2e5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] Block range: 11043877 ~ 20472970\n",
      "[INFO] Blocks 11043877â€“11243876, Page 1, Added 3828, Total 3828\n",
      "[INFO] Blocks 11243877â€“11443876, Page 1, Added 238, Total 4066\n",
      "[INFO] Blocks 11443877â€“11643876, Page 1, Added 321, Total 4387\n",
      "[INFO] Blocks 11643877â€“11843876, Page 1, Added 2973, Total 7360\n",
      "[INFO] Blocks 11843877â€“12043876, Page 1, Added 2121, Total 9481\n",
      "[INFO] Blocks 12043877â€“12243876, Page 1, Added 5185, Total 14666\n",
      "[INFO] Blocks 12243877â€“12443876, Page 1, Added 2273, Total 16939\n",
      "[INFO] Blocks 12443877â€“12643876, Page 1, Added 1210, Total 18149\n",
      "[INFO] Blocks 12643877â€“12843876, Page 1, Added 3398, Total 21547\n",
      "[INFO] Blocks 12843877â€“13043876, Page 1, Added 1086, Total 22633\n",
      "[INFO] Blocks 13043877â€“13243876, Page 1, Added 220, Total 22853\n",
      "[INFO] Blocks 13243877â€“13443876, Page 1, Added 144, Total 22997\n",
      "[INFO] Blocks 13443877â€“13643876, Page 1, Added 3338, Total 26335\n",
      "[INFO] Blocks 13643877â€“13843876, Page 1, Added 38, Total 26373\n",
      "[INFO] Blocks 13843877â€“14043876, Page 1, Added 10, Total 26383\n",
      "[INFO] Blocks 14043877â€“14243876, Page 1, Added 9, Total 26392\n",
      "[INFO] Blocks 14243877â€“14443876, Page 1, Added 36, Total 26428\n",
      "[INFO] Blocks 14443877â€“14643876, Page 1, Added 12, Total 26440\n",
      "[INFO] Blocks 14643877â€“14843876, Page 1, Added 8, Total 26448\n",
      "[INFO] Blocks 14843877â€“15043876, Page 1, Added 2, Total 26450\n",
      "[INFO] Blocks 15043877â€“15243876, Page 1, Added 18, Total 26468\n",
      "[INFO] Blocks 15243877â€“15443876, Page 1, Added 9, Total 26477\n",
      "[INFO] Blocks 15443877â€“15643876, Page 1, Added 8, Total 26485\n",
      "[INFO] Blocks 15643877â€“15843876, Page 1, Added 3, Total 26488\n",
      "[INFO] Blocks 15843877â€“16043876, Page 1, Added 11, Total 26499\n",
      "[INFO] Blocks 16043877â€“16243876, Page 1, Added 22, Total 26521\n",
      "[INFO] Blocks 16243877â€“16443876, Page 1, Added 13, Total 26534\n",
      "[INFO] Blocks 16443877â€“16643876, Page 1, Added 5, Total 26539\n",
      "[INFO] Blocks 16643877â€“16843876, Page 1, Added 14, Total 26553\n",
      "[INFO] Blocks 16843877â€“17043876, Page 1, NO TX\n",
      "[INFO] Blocks 17043877â€“17243876, Page 1, Added 2, Total 26555\n",
      "[INFO] Blocks 17243877â€“17443876, Page 1, NO TX\n",
      "[INFO] Blocks 17443877â€“17643876, Page 1, NO TX\n",
      "[INFO] Blocks 17643877â€“17843876, Page 1, Added 1, Total 26556\n",
      "[INFO] Blocks 17843877â€“18043876, Page 1, NO TX\n",
      "[INFO] Blocks 18043877â€“18243876, Page 1, NO TX\n",
      "[INFO] Blocks 18243877â€“18443876, Page 1, NO TX\n",
      "[INFO] Blocks 18443877â€“18643876, Page 1, NO TX\n",
      "[INFO] Blocks 18643877â€“18843876, Page 1, Added 2, Total 26558\n",
      "[INFO] Blocks 18843877â€“19043876, Page 1, Added 8, Total 26566\n",
      "[INFO] Blocks 19043877â€“19243876, Page 1, Added 2, Total 26568\n",
      "[INFO] Blocks 19243877â€“19443876, Page 1, Added 2, Total 26570\n",
      "[INFO] Blocks 19443877â€“19643876, Page 1, Added 8, Total 26578\n",
      "[INFO] Blocks 19643877â€“19843876, Page 1, Added 4, Total 26582\n",
      "[INFO] Blocks 19843877â€“20043876, Page 1, Added 6, Total 26588\n",
      "[INFO] Blocks 20043877â€“20243876, Page 1, NO TX\n",
      "[INFO] Blocks 20243877â€“20443876, Page 1, Added 4, Total 26592\n",
      "[INFO] Blocks 20443877â€“20472970, Page 1, NO TX\n",
      "[INFO] Columns: ['blockNumber', 'timeStamp', 'hash', 'nonce', 'blockHash', 'from', 'contractAddress', 'to', 'value', 'tokenName', 'tokenSymbol', 'tokenDecimal', 'transactionIndex', 'gas', 'gasPrice', 'gasUsed', 'cumulativeGasUsed', 'input', 'methodId', 'functionName', 'confirmations']\n",
      "[INFO] Finished. Total collected: 25603 transactions â†’ picaartmoney_transactions_full.csv\n",
      "[INFO] No block range gaps in iteration (every chunk covered).\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "9e549a34",
   "metadata": {},
   "source": [
    "### ìˆ˜ì§‘ ë°ì´í„° ì „ì²˜ë¦¬"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "99cd43a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# 0) Imports\n",
    "# ============================================================\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from torch import nn\n",
    "\n",
    "from torch_geometric.data import Data\n",
    "from torch_geometric.nn import GATConv, GAE\n",
    "from torch_geometric.utils import to_undirected, negative_sampling\n",
    "from torch_geometric.transforms import RandomLinkSplit\n",
    "\n",
    "import networkx as nx\n",
    "from decimal import Decimal, getcontext\n",
    "from datetime import datetime, timezone\n",
    "from collections import defaultdict, Counter\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "718a3c99",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# ------------------------------------------------------------\n",
    "# íŒŒì¼ ê²½ë¡œ\n",
    "CSV_PATH = \"picaartmoney_transactions_full.csv\"\n",
    "USE_COLS = [\n",
    "    'blockNumber','timeStamp','hash','nonce','blockHash','from','contractAddress','to','value',\n",
    "    'tokenName','tokenSymbol','tokenDecimal','transactionIndex','gas','gasPrice','gasUsed',\n",
    "    'cumulativeGasUsed','input','confirmations'\n",
    "]\n",
    "\n",
    "# ì†Œìˆ˜ ì •ë°€ë„ (ETH/í† í° ì†Œìˆ˜ ì²˜ë¦¬ ì•ˆì „í•˜ê²Œ)\n",
    "getcontext().prec = 50"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "7cc58294",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>blockNumber</th>\n",
       "      <th>timeStamp</th>\n",
       "      <th>hash</th>\n",
       "      <th>nonce</th>\n",
       "      <th>blockHash</th>\n",
       "      <th>from</th>\n",
       "      <th>contractAddress</th>\n",
       "      <th>to</th>\n",
       "      <th>value</th>\n",
       "      <th>tokenName</th>\n",
       "      <th>...</th>\n",
       "      <th>transactionIndex</th>\n",
       "      <th>gas</th>\n",
       "      <th>gasPrice</th>\n",
       "      <th>gasUsed</th>\n",
       "      <th>cumulativeGasUsed</th>\n",
       "      <th>input</th>\n",
       "      <th>confirmations</th>\n",
       "      <th>timestamp_dt</th>\n",
       "      <th>value_float</th>\n",
       "      <th>gas_used</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>11085582</td>\n",
       "      <td>1603098539</td>\n",
       "      <td>0x328008e17407b6ba014295bfe5069ba4f60f1296aea0...</td>\n",
       "      <td>0</td>\n",
       "      <td>0x7185066660404b22f7f1cac0862c8a7f5c1ef99e4a1f...</td>\n",
       "      <td>0x0000000000000000000000000000000000000000</td>\n",
       "      <td>0xa7e0719a65128b2f6cdbc86096753ff7d5962106</td>\n",
       "      <td>0xd28493e737fbcc957f3716143ed6e40f40357b51</td>\n",
       "      <td>1000000000</td>\n",
       "      <td>PicaArtMoney</td>\n",
       "      <td>...</td>\n",
       "      <td>251</td>\n",
       "      <td>1603895</td>\n",
       "      <td>36000000000</td>\n",
       "      <td>1603895</td>\n",
       "      <td>11088223</td>\n",
       "      <td>deprecated</td>\n",
       "      <td>12287913</td>\n",
       "      <td>2020-10-19 09:08:59+00:00</td>\n",
       "      <td>1000000000</td>\n",
       "      <td>1603895</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>11091306</td>\n",
       "      <td>1603174037</td>\n",
       "      <td>0xa8352e8094fb444e9bfa9a4a6d8011502a0f4655ad38...</td>\n",
       "      <td>1</td>\n",
       "      <td>0x6902d2e0773fd0bad116b753b98e37d61d7a4b532d71...</td>\n",
       "      <td>0xd28493e737fbcc957f3716143ed6e40f40357b51</td>\n",
       "      <td>0xa7e0719a65128b2f6cdbc86096753ff7d5962106</td>\n",
       "      <td>0xfa9b57cbe5b7bd63b436dcf205c15222b510ff27</td>\n",
       "      <td>150000000</td>\n",
       "      <td>PicaArtMoney</td>\n",
       "      <td>...</td>\n",
       "      <td>185</td>\n",
       "      <td>53110</td>\n",
       "      <td>26000000000</td>\n",
       "      <td>53110</td>\n",
       "      <td>12360057</td>\n",
       "      <td>deprecated</td>\n",
       "      <td>12282189</td>\n",
       "      <td>2020-10-20 06:07:17+00:00</td>\n",
       "      <td>150000000</td>\n",
       "      <td>53110</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>11091306</td>\n",
       "      <td>1603174037</td>\n",
       "      <td>0x36195c14399493bdf43387ded77d87d3b5e98c94a138...</td>\n",
       "      <td>2</td>\n",
       "      <td>0x6902d2e0773fd0bad116b753b98e37d61d7a4b532d71...</td>\n",
       "      <td>0xd28493e737fbcc957f3716143ed6e40f40357b51</td>\n",
       "      <td>0xa7e0719a65128b2f6cdbc86096753ff7d5962106</td>\n",
       "      <td>0x32f042b0b01f10247493a950456f4c4304d46ba5</td>\n",
       "      <td>150000000</td>\n",
       "      <td>PicaArtMoney</td>\n",
       "      <td>...</td>\n",
       "      <td>186</td>\n",
       "      <td>53110</td>\n",
       "      <td>26000000000</td>\n",
       "      <td>53110</td>\n",
       "      <td>12413167</td>\n",
       "      <td>deprecated</td>\n",
       "      <td>12282189</td>\n",
       "      <td>2020-10-20 06:07:17+00:00</td>\n",
       "      <td>150000000</td>\n",
       "      <td>53110</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>11091306</td>\n",
       "      <td>1603174037</td>\n",
       "      <td>0xbf52a2c6de897287b5c6740c891a71a5122c69566bdb...</td>\n",
       "      <td>3</td>\n",
       "      <td>0x6902d2e0773fd0bad116b753b98e37d61d7a4b532d71...</td>\n",
       "      <td>0xd28493e737fbcc957f3716143ed6e40f40357b51</td>\n",
       "      <td>0xa7e0719a65128b2f6cdbc86096753ff7d5962106</td>\n",
       "      <td>0xd4b394c60bb55f80df30dac87b6f92be34739332</td>\n",
       "      <td>300000000</td>\n",
       "      <td>PicaArtMoney</td>\n",
       "      <td>...</td>\n",
       "      <td>187</td>\n",
       "      <td>53098</td>\n",
       "      <td>26000000000</td>\n",
       "      <td>53098</td>\n",
       "      <td>12466265</td>\n",
       "      <td>deprecated</td>\n",
       "      <td>12282189</td>\n",
       "      <td>2020-10-20 06:07:17+00:00</td>\n",
       "      <td>300000000</td>\n",
       "      <td>53098</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>11091313</td>\n",
       "      <td>1603174087</td>\n",
       "      <td>0x246a4998919d4c369ff0735ea372729a9e06199055e1...</td>\n",
       "      <td>4</td>\n",
       "      <td>0xe18ff0b2e114d21a3f5cf8c2976342cea2fc558254ee...</td>\n",
       "      <td>0xd28493e737fbcc957f3716143ed6e40f40357b51</td>\n",
       "      <td>0xa7e0719a65128b2f6cdbc86096753ff7d5962106</td>\n",
       "      <td>0xc32b1345acae345c595d3bbcf62e14e5f3020456</td>\n",
       "      <td>200000000</td>\n",
       "      <td>PicaArtMoney</td>\n",
       "      <td>...</td>\n",
       "      <td>100</td>\n",
       "      <td>53098</td>\n",
       "      <td>27000000000</td>\n",
       "      <td>53098</td>\n",
       "      <td>5923677</td>\n",
       "      <td>deprecated</td>\n",
       "      <td>12282182</td>\n",
       "      <td>2020-10-20 06:08:07+00:00</td>\n",
       "      <td>200000000</td>\n",
       "      <td>53098</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 22 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "  blockNumber   timeStamp                                               hash  \\\n",
       "0    11085582  1603098539  0x328008e17407b6ba014295bfe5069ba4f60f1296aea0...   \n",
       "1    11091306  1603174037  0xa8352e8094fb444e9bfa9a4a6d8011502a0f4655ad38...   \n",
       "2    11091306  1603174037  0x36195c14399493bdf43387ded77d87d3b5e98c94a138...   \n",
       "3    11091306  1603174037  0xbf52a2c6de897287b5c6740c891a71a5122c69566bdb...   \n",
       "4    11091313  1603174087  0x246a4998919d4c369ff0735ea372729a9e06199055e1...   \n",
       "\n",
       "  nonce                                          blockHash  \\\n",
       "0     0  0x7185066660404b22f7f1cac0862c8a7f5c1ef99e4a1f...   \n",
       "1     1  0x6902d2e0773fd0bad116b753b98e37d61d7a4b532d71...   \n",
       "2     2  0x6902d2e0773fd0bad116b753b98e37d61d7a4b532d71...   \n",
       "3     3  0x6902d2e0773fd0bad116b753b98e37d61d7a4b532d71...   \n",
       "4     4  0xe18ff0b2e114d21a3f5cf8c2976342cea2fc558254ee...   \n",
       "\n",
       "                                         from  \\\n",
       "0  0x0000000000000000000000000000000000000000   \n",
       "1  0xd28493e737fbcc957f3716143ed6e40f40357b51   \n",
       "2  0xd28493e737fbcc957f3716143ed6e40f40357b51   \n",
       "3  0xd28493e737fbcc957f3716143ed6e40f40357b51   \n",
       "4  0xd28493e737fbcc957f3716143ed6e40f40357b51   \n",
       "\n",
       "                              contractAddress  \\\n",
       "0  0xa7e0719a65128b2f6cdbc86096753ff7d5962106   \n",
       "1  0xa7e0719a65128b2f6cdbc86096753ff7d5962106   \n",
       "2  0xa7e0719a65128b2f6cdbc86096753ff7d5962106   \n",
       "3  0xa7e0719a65128b2f6cdbc86096753ff7d5962106   \n",
       "4  0xa7e0719a65128b2f6cdbc86096753ff7d5962106   \n",
       "\n",
       "                                           to       value     tokenName  ...  \\\n",
       "0  0xd28493e737fbcc957f3716143ed6e40f40357b51  1000000000  PicaArtMoney  ...   \n",
       "1  0xfa9b57cbe5b7bd63b436dcf205c15222b510ff27   150000000  PicaArtMoney  ...   \n",
       "2  0x32f042b0b01f10247493a950456f4c4304d46ba5   150000000  PicaArtMoney  ...   \n",
       "3  0xd4b394c60bb55f80df30dac87b6f92be34739332   300000000  PicaArtMoney  ...   \n",
       "4  0xc32b1345acae345c595d3bbcf62e14e5f3020456   200000000  PicaArtMoney  ...   \n",
       "\n",
       "  transactionIndex      gas     gasPrice  gasUsed  cumulativeGasUsed  \\\n",
       "0              251  1603895  36000000000  1603895           11088223   \n",
       "1              185    53110  26000000000    53110           12360057   \n",
       "2              186    53110  26000000000    53110           12413167   \n",
       "3              187    53098  26000000000    53098           12466265   \n",
       "4              100    53098  27000000000    53098            5923677   \n",
       "\n",
       "        input confirmations              timestamp_dt value_float gas_used  \n",
       "0  deprecated      12287913 2020-10-19 09:08:59+00:00  1000000000  1603895  \n",
       "1  deprecated      12282189 2020-10-20 06:07:17+00:00   150000000    53110  \n",
       "2  deprecated      12282189 2020-10-20 06:07:17+00:00   150000000    53110  \n",
       "3  deprecated      12282189 2020-10-20 06:07:17+00:00   300000000    53098  \n",
       "4  deprecated      12282182 2020-10-20 06:08:07+00:00   200000000    53098  \n",
       "\n",
       "[5 rows x 22 columns]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# ------------------------------------------------------------\n",
    "# 0) CSV ë¡œë“œ & ì „ì²˜ë¦¬\n",
    "df = pd.read_csv(CSV_PATH, usecols=USE_COLS, dtype=str)\n",
    "\n",
    "# ê²°ì¸¡/ê³µë°± ì•ˆì „ ì²˜ë¦¬\n",
    "for col in ['from', 'to']:\n",
    "    df[col] = df[col].fillna('').str.strip().str.lower()\n",
    "\n",
    "# timestamp -> int / datetime\n",
    "df['timeStamp'] = pd.to_numeric(df['timeStamp'], errors='coerce')\n",
    "df = df.dropna(subset=['timeStamp'])\n",
    "df['timestamp_dt'] = pd.to_datetime(df['timeStamp'], unit='s', utc=True)\n",
    "\n",
    "# ìˆ«ìí˜• ì»¬ëŸ¼ ë³€í™˜\n",
    "for c in ['value', 'tokenDecimal', 'gas', 'gasPrice', 'gasUsed']:\n",
    "    df[c] = pd.to_numeric(df[c], errors='coerce').fillna(0)\n",
    "\n",
    "# value â†’ ì •ê·œí™” (value / 10**tokenDecimal)\n",
    "def safe_value(row):\n",
    "    try:\n",
    "        dec = int(row['tokenDecimal'])\n",
    "        val = Decimal(int(row['value'])) / (Decimal(10) ** dec)\n",
    "        return val\n",
    "    except Exception:\n",
    "        return Decimal(0)\n",
    "\n",
    "df['value_float'] = df.apply(safe_value, axis=1)\n",
    "\n",
    "# gasUsed ìš°ì„  ì‚¬ìš© (ì—†ìœ¼ë©´ gas)\n",
    "df['gas_used'] = np.where(df['gasUsed'] > 0, df['gasUsed'], df['gas'])\n",
    "\n",
    "# ì†¡/ìˆ˜ì‹ ì´ ë¹„ì–´ìˆê±°ë‚˜ ë™ì¼ì§€ê°‘ ìê¸°ì „ì†¡ì€ ì œì™¸(ì›í•˜ë©´ í¬í•¨ ê°€ëŠ¥)\n",
    "df = df[(df['from'] != '') & (df['to'] != '') & (df['from'] != df['to'])]\n",
    "\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ecdf681b",
   "metadata": {},
   "source": [
    "### ê·¸ë˜í”„ë°ì´í„° ìƒì„±"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "be752bc2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1) ê¸°ë³¸ì—£ì§€ ê·¸ë˜í”„ (MultiDiGraph) êµ¬ì¶•\n",
    "G_base = nx.MultiDiGraph()\n",
    "# ë…¸ë“œ ì¶”ê°€(ì§€ê°‘ì£¼ì†Œ)\n",
    "nodes = pd.unique(pd.concat([df['from'], df['to']], ignore_index=True))\n",
    "G_base.add_nodes_from(nodes)\n",
    "\n",
    "# ì—£ì§€ ì¶”ê°€(íŠ¸ëœì­ì…˜ ë‹¨ìœ„)\n",
    "for _, r in df.iterrows():\n",
    "    G_base.add_edge(\n",
    "        r['from'], r['to'],\n",
    "        key=r['hash'],\n",
    "        hash=r['hash'],\n",
    "        timestamp=int(r['timeStamp']),\n",
    "        timestamp_dt=r['timestamp_dt'],\n",
    "        value_float=r['value_float'],\n",
    "        gas_used=int(r['gas_used'])\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6803b9f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2) ì´ê´„ì—£ì§€ ê·¸ë˜í”„ (ë¬´ë°©í–¥, ê³„ì¢ŒìŒ ì§‘ê³„)\n",
    "# net_value: sum(A->B) - sum(B->A), tx_count: ì´ ê±°ë˜ íšŸìˆ˜, first_tx_time: ìµœì†Œ ì‹œê°\n",
    "pair_sum_ab = defaultdict(Decimal)   # sum A->B\n",
    "pair_sum_ba = defaultdict(Decimal)   # sum B->A\n",
    "pair_count = Counter()\n",
    "pair_first_ts = dict()\n",
    "\n",
    "for _, r in df.iterrows():\n",
    "    a, b = r['from'], r['to']\n",
    "    key = tuple(sorted((a, b)))\n",
    "    pair_count[key] += 1\n",
    "    ts = int(r['timeStamp'])\n",
    "    if key not in pair_first_ts or ts < pair_first_ts[key]:\n",
    "        pair_first_ts[key] = ts\n",
    "    # ë°©í–¥ í•©ê³„\n",
    "    if a < b:\n",
    "        # ì €ì¥ì€ ì‘ì€ì£¼ì†Œ,í°ì£¼ì†Œ ê¸°ì¤€ìœ¼ë¡œ í•´ë‘ê³  ë°©í–¥ì€ ë”°ë¡œ ê¸°ë¡\n",
    "        pair_sum_ab[key] += r['value_float']  # a(ì‘ì€?)â†’b(í°?)ê°€ ì•„ë‹ ìˆ˜ ìˆì–´ ì•„ë˜ì—ì„œ ë‹¤ì‹œ ë°©í–¥íŒë‹¨\n",
    "    else:\n",
    "        pair_sum_ba[key] += r['value_float']\n",
    "\n",
    "# ë” ëª…í™•í•˜ê²Œ ì¬ì§‘ê³„:\n",
    "pair_dir_sum = defaultdict(lambda: {'ab': Decimal(0), 'ba': Decimal(0)})\n",
    "for _, r in df.iterrows():\n",
    "    a, b = r['from'], r['to']\n",
    "    key = tuple(sorted((a, b)))\n",
    "    if (a, b) == key:\n",
    "        pair_dir_sum[key]['ab'] += r['value_float']\n",
    "    else:\n",
    "        pair_dir_sum[key]['ba'] += r['value_float']\n",
    "\n",
    "H_summary = nx.Graph()\n",
    "H_summary.add_nodes_from(nodes)\n",
    "\n",
    "for key in pair_count.keys():\n",
    "    a, b = key\n",
    "    ab = pair_dir_sum[key]['ab']\n",
    "    ba = pair_dir_sum[key]['ba']\n",
    "    net_value = ab - ba\n",
    "    first_ts = pair_first_ts[key]\n",
    "    H_summary.add_edge(\n",
    "        a, b,\n",
    "        net_value=float(net_value),         # CSV ì €ì¥ í¸ì˜ ìœ„í•´ float ìºìŠ¤íŒ…(ì •ë°€ ë³´ì¡´ ì›í•˜ë©´ str(net_value))\n",
    "        tx_count=int(pair_count[key]),\n",
    "        first_tx_time=int(first_ts)\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bdf3eeaf",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# 3) ë…¸ë“œ íŠ¹ì„± ê³„ì‚° (29ê°œ)\n",
    "# ì¤€ë¹„: ë°©í–¥ ê¸°ì¤€ë³„ ì§‘ê³„\n",
    "# ë…¸ë“œë³„ in/out íŠ¸ëœì­ì…˜ ìˆ˜/ê¸ˆì•¡/ê°€ìŠ¤\n",
    "in_count = Counter()\n",
    "out_count = Counter()\n",
    "in_amount_sum = defaultdict(Decimal)\n",
    "out_amount_sum = defaultdict(Decimal)\n",
    "in_gas_sum = Counter()\n",
    "out_gas_sum = Counter()\n",
    "\n",
    "# ë…¸ë“œë³„ íƒ€ì„ìŠ¤íƒ¬í”„ ëª¨ìŒ\n",
    "node_timestamps = defaultdict(list)\n",
    "\n",
    "# ë…¸ë“œë³„ in/out ì´ì›ƒ(ìƒëŒ€ë°©) ì§‘í•©\n",
    "in_neighbors = defaultdict(set)\n",
    "out_neighbors = defaultdict(set)\n",
    "\n",
    "for _, r in df.iterrows():\n",
    "    s, t = r['from'], r['to']\n",
    "    val = r['value_float']\n",
    "    gasu = int(r['gas_used'])\n",
    "    ts = int(r['timeStamp'])\n",
    "\n",
    "    # ì¹´ìš´íŠ¸/í•©ê³„\n",
    "    out_count[s] += 1\n",
    "    in_count[t] += 1\n",
    "    out_amount_sum[s] += val\n",
    "    in_amount_sum[t] += val\n",
    "    out_gas_sum[s] += gasu\n",
    "    in_gas_sum[t] += gasu\n",
    "\n",
    "    # ì´ì›ƒ\n",
    "    out_neighbors[s].add(t)\n",
    "    in_neighbors[t].add(s)\n",
    "\n",
    "    # íƒ€ì„ìŠ¤íƒ¬í”„\n",
    "    node_timestamps[s].append(ts)\n",
    "    node_timestamps[t].append(ts)\n",
    "\n",
    "# 3-1) ìœ ì¼ ì†¡/ìˆ˜ì‹  ìƒëŒ€ ê´€ë ¨\n",
    "# - \"ì†¡ì‹  íŠ¸ëœì­ì…˜ì˜ ìœ ì¼í•œ ìƒëŒ€ë°©ì´ í•´ë‹¹ ë…¸ë“œì¸ ë…¸ë“œë“¤ì˜ ìˆ˜ / ì´ ìˆ˜ëŸ‰\"\n",
    "#   ì¦‰, Xì˜ out_neighbors(X)ê°€ {v}ì¸ ëª¨ë“  Xë¥¼ v ê¸°ì¤€ìœ¼ë¡œ ì§‘ê³„\n",
    "unique_sender_targets = defaultdict(list)  # v -> [X,...] where X sends only to v\n",
    "for x, outs in out_neighbors.items():\n",
    "    if len(outs) == 1:\n",
    "        v = next(iter(outs))\n",
    "        unique_sender_targets[v].append(x)\n",
    "\n",
    "# ìœ ì¼ì†¡ì‹  ì´ ìˆ˜ëŸ‰(í•´ë‹¹ X->vë¡œ ë³´ë‚¸ ëª¨ë“  ê¸ˆì•¡ í•©)\n",
    "unique_send_total_amount_to_v = defaultdict(Decimal)\n",
    "for v, senders in unique_sender_targets.items():\n",
    "    # df í•„í„° ë¹„ìš© ì¤„ì´ë ¤ë©´ ì‚¬ì „ ì§‘ê³„ë¥¼ ì“°ëŠ” ê²ƒì´ ì¢‹ì§€ë§Œ, ë°ì´í„° ê±´ìˆ˜ê°€ 2.6ë§Œì´ë¼ë„ ì¶©ë¶„íˆ ì²˜ë¦¬ ê°€ëŠ¥\n",
    "    mask = (df['from'].isin(senders)) & (df['to'] == v)\n",
    "    if mask.any():\n",
    "        unique_send_total_amount_to_v[v] = df.loc[mask, 'value_float'].sum()\n",
    "    else:\n",
    "        unique_send_total_amount_to_v[v] = Decimal(0)\n",
    "\n",
    "# - \"ìˆ˜ì‹  íŠ¸ëœì­ì…˜ì˜ ìœ ì¼í•œ ìƒëŒ€ë°œì´ í•´ë‹¹ ë…¸ë“œì¸ ë…¸ë“œë“¤ì˜ ìˆ˜ / ì´ ìˆ˜ëŸ‰\"\n",
    "#   ì¦‰, Yì˜ in_neighbors(Y)ê°€ {v}ì¸ ëª¨ë“  Yë¥¼ v ê¸°ì¤€ìœ¼ë¡œ ì§‘ê³„ (YëŠ” vì—ê²Œì„œë§Œ ë°›ìŒ)\n",
    "unique_receiver_sources = defaultdict(list)  # v -> [Y,...] where Y receives only from v\n",
    "for y, ins in in_neighbors.items():\n",
    "    if len(ins) == 1:\n",
    "        v = next(iter(ins))\n",
    "        unique_receiver_sources[v].append(y)\n",
    "\n",
    "unique_recv_total_amount_from_v = defaultdict(Decimal)\n",
    "for v, receivers in unique_receiver_sources.items():\n",
    "    mask = (df['to'].isin(receivers)) & (df['from'] == v)\n",
    "    if mask.any():\n",
    "        unique_recv_total_amount_from_v[v] = df.loc[mask, 'value_float'].sum()\n",
    "    else:\n",
    "        unique_recv_total_amount_from_v[v] = Decimal(0)\n",
    "\n",
    "# 3-2) ìˆœí™˜ ê±°ë˜ìˆ˜: ê°™ì€ SCC ì•ˆì˜ ìƒëŒ€ì™€ ì£¼ê³ ë°›ì€ íŠ¸ëœì­ì…˜ ìˆ˜\n",
    "# (ë°©í–¥ ê·¸ë˜í”„ì˜ ê°•ê²°í•©ìš”ì†Œ ê¸°ë°˜)\n",
    "sccs = list(nx.strongly_connected_components(G_base))\n",
    "comp_id = {}\n",
    "for i, comp in enumerate(sccs):\n",
    "    for n in comp:\n",
    "        comp_id[n] = i\n",
    "\n",
    "cycle_tx_count = Counter()\n",
    "for u, v, k, d in G_base.edges(keys=True, data=True):\n",
    "    if comp_id.get(u) == comp_id.get(v) and len(sccs[comp_id[u]]) > 1:\n",
    "        cycle_tx_count[u] += 1\n",
    "        cycle_tx_count[v] += 1\n",
    "\n",
    "# 3-3) ì–‘ë°©í–¥ ìƒëŒ€ ìˆ˜: u<->v ëª¨ë‘ ì¡´ì¬í•˜ëŠ” ìƒëŒ€ ìˆ˜\n",
    "bidirectional_count = Counter()\n",
    "# ë¹ ë¥¸ íŒë³„: ë¬´ë°©í–¥ìœ¼ë¡œ ë³€í™˜ í›„, ê° ì´ì›ƒ ì¤‘ ì‹¤ì œë¡œ ì–‘ë°©í–¥ ì¡´ì¬í•˜ëŠ”ì§€ ì²´í¬\n",
    "UG = G_base.to_undirected()\n",
    "for n in G_base.nodes():\n",
    "    cnt = 0\n",
    "    for nbr in UG.neighbors(n):\n",
    "        has_out = G_base.has_edge(n, nbr)\n",
    "        has_in = G_base.has_edge(nbr, n)\n",
    "        if has_out and has_in:\n",
    "            cnt += 1\n",
    "    bidirectional_count[n] = cnt\n",
    "\n",
    "# 3-4) ì¤‘ì‹¬ì„±ë“¤\n",
    "# Degree/Closeness/Betweenness: ë¬´ë°©í–¥ ê·¸ë˜í”„ ê¸°ì¤€\n",
    "deg_centrality = nx.degree_centrality(UG)\n",
    "# closenessëŠ” ì—°ê²°ìš”ì†Œ ë¬¸ì œë¡œ normalized=True ê¸°ë³¸, ë¬´ë°©í–¥ì—ì„œ ê³„ì‚°\n",
    "close_centrality = nx.closeness_centrality(UG)\n",
    "# betweenness: ê³„ì‚° ë¹„ìš© í¼. ë…¸ë“œ ë§ìœ¼ë©´ k-ìƒ˜í”Œë§ ì‚¬ìš© ê³ ë ¤. ì—¬ê¸°ì„œëŠ” ì •í™• ê³„ì‚° ì‹œë„.\n",
    "bet_centrality = nx.betweenness_centrality(UG, normalized=True)\n",
    "\n",
    "# PageRank: ë°©í–¥ + ê°€ì¤‘ì¹˜(value_float)\n",
    "# weightê°€ floatì´ì–´ì•¼ í•´ì„œ ë¯¸ë¦¬ edge attr ì¤€ë¹„ í•„ìš” â†’ ì´ë¯¸ value_float ìˆìŒ\n",
    "# MultiDiGraphì´ë¯€ë¡œ ê°€ì¤‘ì¹˜ í•©ì‚° í•„ìš” â†’ DiGraphë¡œ í•©ì³ì„œ ê°€ì¤‘ì¹˜ ëˆ„ì \n",
    "DG_weighted = nx.DiGraph()\n",
    "for u, v, d in G_base.edges(data=True):\n",
    "    w = float(d.get('value_float', 0))\n",
    "    if w <= 0:\n",
    "        continue\n",
    "    if DG_weighted.has_edge(u, v):\n",
    "        DG_weighted[u][v]['weight'] += w\n",
    "    else:\n",
    "        DG_weighted.add_edge(u, v, weight=w)\n",
    "\n",
    "if DG_weighted.number_of_edges() > 0:\n",
    "    pagerank = nx.pagerank(DG_weighted, weight='weight')\n",
    "else:\n",
    "    pagerank = {n: 0.0 for n in G_base.nodes()}\n",
    "\n",
    "# 3-5) ë‚˜ë¨¸ì§€ ì§€í‘œë“¤ ì¡°ë¦½\n",
    "rows = []\n",
    "for n in G_base.nodes():\n",
    "    in_neigh = in_neighbors.get(n, set())\n",
    "    out_neigh = out_neighbors.get(n, set())\n",
    "\n",
    "    total_in_cnt = in_count.get(n, 0)\n",
    "    total_out_cnt = out_count.get(n, 0)\n",
    "    total_in_amt = in_amount_sum.get(n, Decimal(0))\n",
    "    total_out_amt = out_amount_sum.get(n, Decimal(0))\n",
    "    total_in_gas = in_gas_sum.get(n, 0)\n",
    "    total_out_gas = out_gas_sum.get(n, 0)\n",
    "\n",
    "    # í‰ê· ë“¤(0 ë‚˜ëˆ—ì…ˆ ë°©ì§€)\n",
    "    avg_in_amt = (total_in_amt / total_in_cnt) if total_in_cnt > 0 else Decimal(0)\n",
    "    avg_out_amt = (total_out_amt / total_out_cnt) if total_out_cnt > 0 else Decimal(0)\n",
    "    avg_in_gas = (Decimal(total_in_gas) / total_in_cnt) if total_in_cnt > 0 else Decimal(0)\n",
    "    avg_out_gas = (Decimal(total_out_gas) / total_out_cnt) if total_out_cnt > 0 else Decimal(0)\n",
    "\n",
    "    # ê°€ìŠ¤ íš¨ìœ¨\n",
    "    recv_gas_eff = (total_in_amt / Decimal(total_in_gas)) if total_in_gas > 0 else Decimal(0)\n",
    "    send_gas_eff = (total_out_amt / Decimal(total_out_gas)) if total_out_gas > 0 else Decimal(0)\n",
    "\n",
    "    # í™œë™ ì¼ìˆ˜/ì¼í‰ê· \n",
    "    ts_list = node_timestamps.get(n, [])\n",
    "    unique_days = set()\n",
    "    for ts in ts_list:\n",
    "        d = datetime.fromtimestamp(ts, tz=timezone.utc).date()\n",
    "        unique_days.add(d)\n",
    "    active_days = len(unique_days)\n",
    "    total_tx_cnt = total_in_cnt + total_out_cnt\n",
    "    total_amt_abs = total_in_amt + total_out_amt  # í•„ìš”ì‹œ abs í•©ìœ¼ë¡œ ë°”ê¾¸ê³  ì‹¶ìœ¼ë©´ ìˆ˜ì •\n",
    "    tx_per_day = (total_tx_cnt / active_days) if active_days > 0 else 0\n",
    "    amt_per_day = (total_amt_abs / Decimal(active_days)) if active_days > 0 else Decimal(0)\n",
    "\n",
    "    # ìœ ì¼ ì†¡/ìˆ˜ì‹  ê´€ë ¨\n",
    "    uniq_send_nodes = unique_sender_targets.get(n, [])\n",
    "    uniq_recv_nodes = unique_receiver_sources.get(n, [])\n",
    "\n",
    "    uniq_send_count = len(uniq_send_nodes)\n",
    "    uniq_recv_count = len(uniq_recv_nodes)\n",
    "    uniq_send_amount = unique_send_total_amount_to_v.get(n, Decimal(0))\n",
    "    uniq_recv_amount = unique_recv_total_amount_from_v.get(n, Decimal(0))\n",
    "\n",
    "    # ìˆœí™˜ ê±°ë˜ìˆ˜/ì–‘ë°©í–¥ ìƒëŒ€ìˆ˜\n",
    "    cyc_cnt = cycle_tx_count.get(n, 0)\n",
    "    bidi_cnt = bidirectional_count.get(n, 0)\n",
    "\n",
    "    # ì¤‘ì‹¬ì„±\n",
    "    deg_c = deg_centrality.get(n, 0.0)\n",
    "    clo_c = close_centrality.get(n, 0.0)\n",
    "    bet_c = bet_centrality.get(n, 0.0)\n",
    "    pr = pagerank.get(n, 0.0)\n",
    "\n",
    "    # ì²«/ë§ˆì§€ë§‰ íŠ¸ëœì­ì…˜ ì‹œê°\n",
    "    if ts_list:\n",
    "        first_ts = min(ts_list)\n",
    "        last_ts = max(ts_list)\n",
    "    else:\n",
    "        first_ts = None\n",
    "        last_ts = None\n",
    "\n",
    "    rows.append({\n",
    "        'address': n,\n",
    "\n",
    "        # 1~4 ìœ ì¼ ì†¡/ìˆ˜ì‹  ê´€ë ¨\n",
    "        'unique_sender_nodes_count': uniq_send_count,                          # (1)\n",
    "        'unique_receiver_nodes_count': uniq_recv_count,                        # (2)\n",
    "        'unique_sender_total_amount': float(uniq_send_amount),                 # (3)\n",
    "        'unique_receiver_total_amount': float(uniq_recv_amount),               # (4)\n",
    "\n",
    "        # 5~6 ìˆœí™˜/ì–‘ë°©í–¥\n",
    "        'cycle_tx_count': int(cyc_cnt),                                        # (5)\n",
    "        'bidirectional_counterparties': int(bidi_cnt),                         # (6)\n",
    "\n",
    "        # 7~10 ì¤‘ì‹¬ì„±\n",
    "        'closeness_centrality': float(clo_c),                                  # (7)\n",
    "        'betweenness_centrality': float(bet_c),                                # (8)\n",
    "        'pagerank': float(pr),                                                 # (9)\n",
    "        'degree_centrality': float(deg_c),                                     # (10)\n",
    "\n",
    "        # 11~12 ì´ì›ƒìˆ˜(ë°©í–¥)\n",
    "        'in_neighbor_count': len(in_neigh),                                    # (11)\n",
    "        'out_neighbor_count': len(out_neigh),                                  # (12)\n",
    "\n",
    "        # 13~22 in/out íŠ¸ëœì­ì…˜ ì§‘ê³„\n",
    "        'total_in_tx_count': int(total_in_cnt),                                # (13)\n",
    "        'total_out_tx_count': int(total_out_cnt),                              # (14)\n",
    "        'total_in_amount': float(total_in_amt),                                 # (15)\n",
    "        'total_out_amount': float(total_out_amt),                               # (16)\n",
    "        'avg_in_amount': float(avg_in_amt),                                     # (17)\n",
    "        'avg_out_amount': float(avg_out_amt),                                   # (18)\n",
    "        'total_in_gas_used': int(total_in_gas),                                 # (19)\n",
    "        'total_out_gas_used': int(total_out_gas),                               # (20)\n",
    "        'avg_in_gas_used': float(avg_in_gas),                                   # (21)\n",
    "        'avg_out_gas_used': float(avg_out_gas),                                 # (22)\n",
    "\n",
    "        # 23~25 ì¼ ë‹¨ìœ„\n",
    "        'active_days': int(active_days),                                        # (23)\n",
    "        'tx_per_day': float(tx_per_day),                                        # (24)\n",
    "        'amount_per_day': float(amt_per_day),                                   # (25)\n",
    "\n",
    "        # 26~27 ê°€ìŠ¤ íš¨ìœ¨\n",
    "        'recv_gas_efficiency': float(recv_gas_eff),                             # (26)\n",
    "        'send_gas_efficiency': float(send_gas_eff),                             # (27)\n",
    "\n",
    "        # 28~29 ì²«/ë§ˆì§€ë§‰ ê±°ë˜ì‹œê° (epoch seconds)\n",
    "        'first_tx_time': int(first_ts) if first_ts is not None else None,       # (28)\n",
    "        'last_tx_time': int(last_ts) if last_ts is not None else None           # (29)\n",
    "    })\n",
    "\n",
    "node_features = pd.DataFrame(rows).sort_values('address').reset_index(drop=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "0826c98b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… ì™„ë£Œ: node_features.csv / G_base_multidigraph.pkl / H_summary_graph.pkl ìƒì„±\n"
     ]
    }
   ],
   "source": [
    "# 4) ì‚°ì¶œë¬¼ ì €ì¥\n",
    "node_features.to_csv('node_features.csv', index=False, encoding='utf-8')\n",
    "\n",
    "import pickle\n",
    "with open(\"G_base_multidigraph.pkl\", \"wb\") as f:\n",
    "    pickle.dump(G_base, f)\n",
    "with open(\"H_summary_graph.pkl\", \"wb\") as f:\n",
    "    pickle.dump(H_summary, f)\n",
    "\n",
    "print(\"âœ… ì™„ë£Œ: node_features.csv / G_base_multidigraph.pkl / H_summary_graph.pkl ìƒì„±\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f385290",
   "metadata": {},
   "source": [
    "### Node2vecë°©ì‹ìœ¼ë¡œ ê·¸ë˜í”„êµ¬ì¡°ë§Œ ë²¡í„°ë¡œ ì„ë² ë”©í•˜ì—¬ í•™ìŠµ - GATì•Œê³ ë¦¬ì¦˜"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6219fb5f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DEVICE: cpu | nodes: 7958 | edges: 25601\n",
      "ğŸ› ï¸ Node2Vec ì„ë² ë”©ì„ ìƒˆë¡œ í•™ìŠµí•©ë‹ˆë‹¤...\n",
      "[Node2Vec] epoch 001 | loss 3.0723\n",
      "[Node2Vec] epoch 002 | loss 1.3729\n",
      "[Node2Vec] epoch 003 | loss 0.9691\n",
      "[Node2Vec] epoch 004 | loss 0.8544\n",
      "[Node2Vec] epoch 005 | loss 0.8131\n",
      "âœ… ì €ì¥ ì™„ë£Œ: node2vec_embeddings.npy, edge_index.pt, addresses.csv, node2vec.pt\n",
      "[GAT-AE] epoch 0001 | recon MSE 0.251541\n",
      "[GAT-AE] epoch 0050 | recon MSE 0.093097\n",
      "[GAT-AE] epoch 0100 | recon MSE 0.075579\n",
      "[GAT-AE] epoch 0150 | recon MSE 0.072817\n",
      "[GAT-AE] epoch 0200 | recon MSE 0.073885\n",
      "[GAT-AE] epoch 0250 | recon MSE 0.069703\n",
      "[GAT-AE] epoch 0300 | recon MSE 0.067430\n",
      "[GAT-AE] epoch 0350 | recon MSE 0.066992\n",
      "[GAT-AE] epoch 0400 | recon MSE 0.066373\n",
      "[GAT-AE] epoch 0450 | recon MSE 0.065949\n",
      "[GAT-AE] epoch 0500 | recon MSE 0.066416\n",
      "[GAT-AE] epoch 0550 | recon MSE 0.065492\n",
      "[GAT-AE] epoch 0600 | recon MSE 0.065401\n",
      "[GAT-AE] epoch 0650 | recon MSE 0.065288\n",
      "[GAT-AE] epoch 0700 | recon MSE 0.065069\n",
      "[GAT-AE] epoch 0750 | recon MSE 0.065271\n",
      "[GAT-AE] epoch 0800 | recon MSE 0.065163\n",
      "[GAT-AE] epoch 0850 | recon MSE 0.064657\n",
      "[GAT-AE] epoch 0900 | recon MSE 0.064808\n",
      "[GAT-AE] epoch 0950 | recon MSE 0.065027\n",
      "[GAT-AE] epoch 1000 | recon MSE 0.064793\n",
      "ìƒìœ„ 5% z-score ì„ê³„ê°’: 1.1053\n",
      "ì´ìƒì¹˜ ë…¸ë“œ ìˆ˜: 398 / 7958\n",
      "âœ… ì™„ë£Œ: gat_ae_anomalies.csv / gat_autoencoder.pt ìƒì„±\n",
      "â„¹ï¸ ì„ë² ë”©/ì—£ì§€ëŠ” íŒŒì¼ë¡œ ë³´ê´€ë¨ â†’ node2vec_embeddings.npy, edge_index.pt, addresses.csv (í•„ìš” ì‹œ ì¬ì‚¬ìš©)\n"
     ]
    }
   ],
   "source": [
    "# -*- coding: utf-8 -*-\n",
    "\"\"\"\n",
    "Node2Vec(64d; CPU, ì €ì¥/ì¬ì‚¬ìš©) + GAT ì˜¤í† ì¸ì½”ë”(256->32->64; DEVICE)\n",
    "â†’ ì¬êµ¬ì„±ì˜¤ì°¨ z-score â†’ ìƒìœ„ 5% ì´ìƒì¹˜\n",
    "\n",
    "- Node2Vecì€ CPUì—ì„œë§Œ ìˆ˜í–‰ (edge_indexë„ CPU)\n",
    "- DataLoaderëŠ” num_workers=0 (Windows/Jupyter PyCapsule í”¼í´ë§ ì—ëŸ¬ íšŒí”¼)\n",
    "- ì„ë² ë”©/ì—£ì§€/ì£¼ì†Œë¥¼ ë””ìŠ¤í¬ì— ì €ì¥í•˜ì—¬ ë‹¤ìŒ ì„¸ì…˜ì—ì„œ ì¬ì‚¬ìš©\n",
    "\"\"\"\n",
    "\n",
    "import os\n",
    "import pickle\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import networkx as nx\n",
    "import torch\n",
    "from torch import nn\n",
    "import torch.nn.functional as F\n",
    "from torch_geometric.nn import Node2Vec, GATConv\n",
    "from torch_geometric.data import Data\n",
    "\n",
    "# ------------------------------------------------------------\n",
    "# ê²½ë¡œ/í™˜ê²½\n",
    "NODE_FEAT_CSV = \"node_features.csv\"\n",
    "GRAPH_PKL     = \"G_base_multidigraph.pkl\"\n",
    "\n",
    "EMB_NPY = \"node2vec_embeddings.npy\"  # ì €ì¥ë  ì„ë² ë”©\n",
    "EDGE_PT = \"edge_index.pt\"            # ì €ì¥ë  edge_index(CPU long)\n",
    "ADDR_CSV = \"addresses.csv\"           # ì €ì¥ë  ì£¼ì†Œ ìˆœì„œ\n",
    "N2V_PT  = \"node2vec.pt\"              # (ì„ íƒ) Node2Vec state_dict\n",
    "\n",
    "FORCE_RETRAIN = False  # Trueë¡œ ë°”ê¾¸ë©´ ì„ë² ë”©ì„ ê°•ì œë¡œ ë‹¤ì‹œ í•™ìŠµ\n",
    "\n",
    "DEVICE = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "torch.manual_seed(42)\n",
    "np.random.seed(42)\n",
    "\n",
    "# ------------------------------------------------------------\n",
    "# 0) ë…¸ë“œ/ê·¸ë˜í”„ ë¡œë“œ\n",
    "if not os.path.exists(NODE_FEAT_CSV):\n",
    "    raise FileNotFoundError(f\"{NODE_FEAT_CSV} íŒŒì¼ì´ ì—†ìŠµë‹ˆë‹¤.\")\n",
    "if not os.path.exists(GRAPH_PKL):\n",
    "    raise FileNotFoundError(f\"{GRAPH_PKL} íŒŒì¼ì´ ì—†ìŠµë‹ˆë‹¤.\")\n",
    "\n",
    "node_df = pd.read_csv(NODE_FEAT_CSV)\n",
    "addresses_cur = node_df['address'].astype(str).tolist()\n",
    "addr2idx = {a: i for i, a in enumerate(addresses_cur)}\n",
    "\n",
    "with open(GRAPH_PKL, \"rb\") as f:\n",
    "    G_nx = pickle.load(f)  # networkx.MultiDiGraph\n",
    "\n",
    "# edge_index (CPU í…ì„œ)\n",
    "edges = []\n",
    "for u, v, _k in G_nx.edges(keys=True):\n",
    "    if u in addr2idx and v in addr2idx:\n",
    "        edges.append([addr2idx[u], addr2idx[v]])\n",
    "edge_index_cpu = torch.tensor(edges, dtype=torch.long).t().contiguous()  # [2,E] (CPU)\n",
    "\n",
    "num_nodes = len(addresses_cur)\n",
    "num_edges = edge_index_cpu.size(1)\n",
    "print(f\"DEVICE: {DEVICE} | nodes: {num_nodes} | edges: {num_edges}\")\n",
    "\n",
    "# (ì„ íƒ) ì–‘ë°©í–¥ ì¶”ê°€: ì•ˆì •ì„±/ì„±ëŠ¥ í–¥ìƒì— ë„ì›€\n",
    "# edge_index_cpu = torch.cat([edge_index_cpu, edge_index_cpu.flip(0)], dim=1)\n",
    "\n",
    "# ------------------------------------------------------------\n",
    "# 1) Node2Vec ì„ë² ë”©: ì €ì¥ë³¸ì´ ìˆìœ¼ë©´ ë¡œë“œ, ì—†ìœ¼ë©´ í•™ìŠµ í›„ ì €ì¥\n",
    "use_saved = (not FORCE_RETRAIN) and os.path.exists(EMB_NPY) and os.path.exists(EDGE_PT) and os.path.exists(ADDR_CSV)\n",
    "\n",
    "def load_saved_embeddings():\n",
    "    # ì£¼ì†Œ ìˆœì„œ ì¼ì¹˜ ì—¬ë¶€ ê²€ì¦\n",
    "    addresses_saved = pd.read_csv(ADDR_CSV)['address'].astype(str).tolist()\n",
    "    if len(addresses_saved) != len(addresses_cur) or any(a!=b for a,b in zip(addresses_saved, addresses_cur)):\n",
    "        print(\"âš ï¸ ì €ì¥ëœ addresses.csv ì™€ í˜„ì¬ addresses ìˆœì„œ/ê°œìˆ˜ê°€ ë‹¤ë¦…ë‹ˆë‹¤. ì¬í•™ìŠµìœ¼ë¡œ ì „í™˜í•©ë‹ˆë‹¤.\")\n",
    "        return None, None, None\n",
    "\n",
    "    x_np = np.load(EMB_NPY)  # (N,64)\n",
    "    if x_np.shape[0] != len(addresses_cur) or x_np.shape[1] != 64:\n",
    "        print(\"âš ï¸ ì €ì¥ëœ ì„ë² ë”© í¬ê¸°ê°€ ì˜ˆìƒê³¼ ë‹¤ë¦…ë‹ˆë‹¤. ì¬í•™ìŠµìœ¼ë¡œ ì „í™˜í•©ë‹ˆë‹¤.\")\n",
    "        return None, None, None\n",
    "\n",
    "    edge_idx_saved = torch.load(EDGE_PT, map_location=\"cpu\")\n",
    "    # ê°„ë‹¨ ê²€ì¦: dtype/shape\n",
    "    if edge_idx_saved.dtype != torch.long or edge_idx_saved.dim()!=2 or edge_idx_saved.size(0)!=2:\n",
    "        print(\"âš ï¸ ì €ì¥ëœ edge_index í˜•ì‹ì´ ì˜ˆìƒê³¼ ë‹¤ë¦…ë‹ˆë‹¤. ì¬í•™ìŠµìœ¼ë¡œ ì „í™˜í•©ë‹ˆë‹¤.\")\n",
    "        return None, None, None\n",
    "\n",
    "    print(\"âœ… ì €ì¥ëœ Node2Vec ì„ë² ë”©/ì—£ì§€/ì£¼ì†Œë¥¼ ë¶ˆëŸ¬ì˜µë‹ˆë‹¤.\")\n",
    "    return x_np, edge_idx_saved, addresses_saved\n",
    "\n",
    "x_init_cpu = None\n",
    "if use_saved:\n",
    "    x_np, edge_index_loaded, addresses_saved = load_saved_embeddings()\n",
    "    if x_np is not None:\n",
    "        x_init_cpu = torch.tensor(x_np, dtype=torch.float32)  # (N,64) CPU\n",
    "        edge_index_cpu = edge_index_loaded                    # ì €ì¥ëœ ì—£ì§€ë¡œ êµì²´(ì•ˆì „)\n",
    "        addresses = addresses_saved\n",
    "    else:\n",
    "        use_saved = False  # ë¶ˆì¼ì¹˜ -> ì¬í•™ìŠµ\n",
    "\n",
    "if not use_saved:\n",
    "    # Node2Vec (CPUì—ì„œë§Œ ìˆ˜í–‰)\n",
    "    print(\"ğŸ› ï¸ Node2Vec ì„ë² ë”©ì„ ìƒˆë¡œ í•™ìŠµí•©ë‹ˆë‹¤...\")\n",
    "    data_cpu = Data(edge_index=edge_index_cpu, num_nodes=num_nodes)  # CPU ì „ìš© Data\n",
    "\n",
    "    n2v = Node2Vec(\n",
    "        data_cpu.edge_index,  # CPU í…ì„œ\n",
    "        embedding_dim=64,\n",
    "        walk_length=30,\n",
    "        context_size=10,     # window\n",
    "        walks_per_node=200,  # num_walk\n",
    "        p=1.0, q=1.0,\n",
    "        num_negative_samples=1,\n",
    "        sparse=True          # SparseAdam ì‚¬ìš©\n",
    "    )\n",
    "    n2v_loader = n2v.loader(batch_size=128, shuffle=True, num_workers=0, persistent_workers=False)\n",
    "    n2v_optimizer = torch.optim.SparseAdam(list(n2v.parameters()), lr=0.01)\n",
    "\n",
    "    def train_node2vec(epochs=5):\n",
    "        n2v.train()\n",
    "        for epoch in range(1, epochs + 1):\n",
    "            total_loss = 0.0\n",
    "            for pos_rw, neg_rw in n2v_loader:\n",
    "                n2v_optimizer.zero_grad()\n",
    "                loss = n2v.loss(pos_rw, neg_rw)  # CPU ê²½ë¡œ\n",
    "                loss.backward()\n",
    "                n2v_optimizer.step()\n",
    "                total_loss += loss.item()\n",
    "            print(f\"[Node2Vec] epoch {epoch:03d} | loss {total_loss/len(n2v_loader):.4f}\")\n",
    "\n",
    "    train_node2vec(epochs=5)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        x_init_cpu = n2v.embedding.weight.clone().detach()     # (N,64) CPU\n",
    "\n",
    "    # â”€ ì €ì¥ â”€\n",
    "    np.save(EMB_NPY, x_init_cpu.numpy())\n",
    "    torch.save(edge_index_cpu, EDGE_PT)\n",
    "    pd.Series(addresses_cur, name=\"address\").to_csv(ADDR_CSV, index=False)\n",
    "    try:\n",
    "        torch.save(n2v.state_dict(), N2V_PT)\n",
    "    except Exception as e:\n",
    "        print(f\"(ì°¸ê³ ) Node2Vec state ì €ì¥ ìƒëµ: {e}\")\n",
    "    addresses = addresses_cur\n",
    "    print(f\"âœ… ì €ì¥ ì™„ë£Œ: {EMB_NPY}, {EDGE_PT}, {ADDR_CSV}, {N2V_PT}\")\n",
    "\n",
    "# ìµœì¢… ì…ë ¥ í…ì„œ (DEVICEë¡œ ì´ë™)\n",
    "x_in = x_init_cpu.to(DEVICE)\n",
    "edge_index_dev = edge_index_cpu.to(DEVICE)\n",
    "\n",
    "# ------------------------------------------------------------\n",
    "# 2) GAT ì˜¤í† ì¸ì½”ë” (DEVICEì—ì„œ ìˆ˜í–‰)\n",
    "class GATAutoEncoder(nn.Module):\n",
    "    def __init__(self, in_dim=64, dropout=0.3):\n",
    "        super().__init__()\n",
    "        self.dropout = dropout\n",
    "        # 64 -> (32 * 8) = 256\n",
    "        self.gat1 = GATConv(in_channels=in_dim, out_channels=32, heads=8, concat=True, dropout=dropout)\n",
    "        # 256 -> 32\n",
    "        self.gat2 = GATConv(in_channels=256, out_channels=32, heads=1, concat=True, dropout=dropout)\n",
    "        # 32 -> 64\n",
    "        self.gat3 = GATConv(in_channels=32, out_channels=64, heads=1, concat=True, dropout=dropout)\n",
    "\n",
    "    def forward(self, x, edge_index):\n",
    "        x = F.dropout(x, p=self.dropout, training=self.training)\n",
    "        x = self.gat1(x, edge_index)\n",
    "        x = F.elu(x)\n",
    "\n",
    "        x = F.dropout(x, p=self.dropout, training=self.training)\n",
    "        x = self.gat2(x, edge_index)\n",
    "        x = F.elu(x)\n",
    "\n",
    "        x = F.dropout(x, p=self.dropout, training=self.training)\n",
    "        x = self.gat3(x, edge_index)  # ìµœì¢… ë³µì› 64d\n",
    "        return x\n",
    "\n",
    "model = GATAutoEncoder(in_dim=64, dropout=0.3).to(DEVICE)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=1e-3, weight_decay=5e-4)\n",
    "\n",
    "# ------------------------------------------------------------\n",
    "# 3) í•™ìŠµ (1000 epochs, MSE ì¬êµ¬ì„±ì˜¤ì°¨)\n",
    "def train_gat_ae(epochs=1000):\n",
    "    model.train()\n",
    "    for epoch in range(1, epochs + 1):\n",
    "        optimizer.zero_grad()\n",
    "        x_hat = model(x_in, edge_index_dev)\n",
    "        loss = F.mse_loss(x_hat, x_in)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        if epoch % 50 == 0 or epoch == 1:\n",
    "            print(f\"[GAT-AE] epoch {epoch:04d} | recon MSE {loss.item():.6f}\")\n",
    "\n",
    "train_gat_ae(epochs=1000)\n",
    "\n",
    "# ------------------------------------------------------------\n",
    "# 4) ì¬êµ¬ì„±ì˜¤ì°¨ â†’ z-score â†’ ìƒìœ„ 5% ì´ìƒì¹˜\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    x_recon = model(x_in, edge_index_dev)\n",
    "\n",
    "recon_err = torch.mean((x_recon - x_in) ** 2, dim=1).detach().cpu().numpy()\n",
    "mu = recon_err.mean()\n",
    "sigma = recon_err.std(ddof=1) if recon_err.size > 1 else 1e-8\n",
    "z = (recon_err - mu) / (sigma if sigma > 0 else 1e-8)\n",
    "z_cut = np.percentile(z, 95.0)\n",
    "anom = (z > z_cut).astype(int)\n",
    "\n",
    "print(f\"ìƒìœ„ 5% z-score ì„ê³„ê°’: {z_cut:.4f}\")\n",
    "print(f\"ì´ìƒì¹˜ ë…¸ë“œ ìˆ˜: {anom.sum()} / {len(anom)}\")\n",
    "\n",
    "# ------------------------------------------------------------\n",
    "# 5) ê²°ê³¼ ì €ì¥\n",
    "out = pd.DataFrame({\n",
    "    \"address\": addresses,\n",
    "    \"recon_mse\": recon_err,\n",
    "    \"z_score\": z,\n",
    "    \"is_anomaly_top5pct\": anom\n",
    "})\n",
    "out.sort_values(\"z_score\", ascending=False).to_csv(\"gat_ae_anomalies.csv\", index=False, encoding=\"utf-8\")\n",
    "torch.save(model.state_dict(), \"gat_autoencoder.pt\")\n",
    "\n",
    "print(\"âœ… ì™„ë£Œ: gat_ae_anomalies.csv / gat_autoencoder.pt ìƒì„±\")\n",
    "print(f\"â„¹ï¸ ì„ë² ë”©/ì—£ì§€ëŠ” íŒŒì¼ë¡œ ë³´ê´€ë¨ â†’ {EMB_NPY}, {EDGE_PT}, {ADDR_CSV} (í•„ìš” ì‹œ ì¬ì‚¬ìš©)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0744e0fd",
   "metadata": {},
   "source": [
    "### Node2vecë°©ì‹ìœ¼ë¡œ ê·¸ë˜í”„êµ¬ì¡°ë§Œ ë²¡í„°ë¡œ ì„ë² ë”©í•˜ì—¬ í•™ìŠµ - GraphSAGEì•Œê³ ë¦¬ì¦˜"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "27561c37",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DEVICE: cpu | nodes: 7958 | edges: 25601\n",
      "âœ… ì €ì¥ëœ Node2Vec ì„ë² ë”©/ì—£ì§€/ì£¼ì†Œë¥¼ ë¶ˆëŸ¬ì˜µë‹ˆë‹¤.\n",
      "[SAGE-AE] epoch 0001 | recon MSE 0.250588\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\user\\AppData\\Local\\Temp\\ipykernel_30980\\1176852788.py:82: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  edge_idx_saved = torch.load(EDGE_PT, map_location=\"cpu\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[SAGE-AE] epoch 0050 | recon MSE 0.130431\n",
      "[SAGE-AE] epoch 0100 | recon MSE 0.090441\n",
      "[SAGE-AE] epoch 0150 | recon MSE 0.085147\n",
      "[SAGE-AE] epoch 0200 | recon MSE 0.074807\n",
      "[SAGE-AE] epoch 0250 | recon MSE 0.068855\n",
      "[SAGE-AE] epoch 0300 | recon MSE 0.067581\n",
      "[SAGE-AE] epoch 0350 | recon MSE 0.067303\n",
      "[SAGE-AE] epoch 0400 | recon MSE 0.064358\n",
      "[SAGE-AE] epoch 0450 | recon MSE 0.063148\n",
      "[SAGE-AE] epoch 0500 | recon MSE 0.063130\n",
      "[SAGE-AE] epoch 0550 | recon MSE 0.062990\n",
      "[SAGE-AE] epoch 0600 | recon MSE 0.062119\n",
      "[SAGE-AE] epoch 0650 | recon MSE 0.061254\n",
      "[SAGE-AE] epoch 0700 | recon MSE 0.060762\n",
      "[SAGE-AE] epoch 0750 | recon MSE 0.060542\n",
      "[SAGE-AE] epoch 0800 | recon MSE 0.061159\n",
      "[SAGE-AE] epoch 0850 | recon MSE 0.059888\n",
      "[SAGE-AE] epoch 0900 | recon MSE 0.059822\n",
      "[SAGE-AE] epoch 0950 | recon MSE 0.059361\n",
      "[SAGE-AE] epoch 1000 | recon MSE 0.059117\n",
      "ìƒìœ„ 5% z-score ì„ê³„ê°’: 1.1954\n",
      "ì´ìƒì¹˜ ë…¸ë“œ ìˆ˜: 398 / 7958\n",
      "âœ… ì™„ë£Œ: sage_ae_anomalies.csv / sage_autoencoder.pt ìƒì„±\n",
      "â„¹ï¸ ì„ë² ë”©/ì—£ì§€ëŠ” íŒŒì¼ë¡œ ë³´ê´€ë¨ â†’ node2vec_embeddings.npy, edge_index.pt, addresses.csv (í•„ìš” ì‹œ ì¬ì‚¬ìš©)\n"
     ]
    }
   ],
   "source": [
    "# -*- coding: utf-8 -*-\n",
    "\"\"\"\n",
    "Node2Vec(64d; CPU, ì €ì¥/ì¬ì‚¬ìš©) + GraphSAGE ì˜¤í† ì¸ì½”ë”(256->32->64; DEVICE)\n",
    "â†’ ì¬êµ¬ì„±ì˜¤ì°¨ z-score â†’ ìƒìœ„ 5% ì´ìƒì¹˜\n",
    "\n",
    "- Node2Vecì€ CPUì—ì„œë§Œ ìˆ˜í–‰ (edge_indexë„ CPU)\n",
    "- DataLoader num_workers=0 (Windows/Jupyter PyCapsule í”¼í´ë§ ì—ëŸ¬ íšŒí”¼)\n",
    "- ì„ë² ë”©/ì—£ì§€/ì£¼ì†Œë¥¼ ë””ìŠ¤í¬ì— ì €ì¥í•˜ì—¬ ë‹¤ìŒ ì„¸ì…˜ì—ì„œ ì¬ì‚¬ìš©\n",
    "\"\"\"\n",
    "\n",
    "import os\n",
    "import pickle\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import networkx as nx\n",
    "import torch\n",
    "from torch import nn\n",
    "import torch.nn.functional as F\n",
    "from torch_geometric.nn import Node2Vec, SAGEConv\n",
    "from torch_geometric.data import Data\n",
    "\n",
    "# ------------------------------------------------------------\n",
    "# ê²½ë¡œ/í™˜ê²½\n",
    "NODE_FEAT_CSV = \"node_features.csv\"\n",
    "GRAPH_PKL     = \"G_base_multidigraph.pkl\"\n",
    "\n",
    "# Node2Vec ì‚°ì¶œë¬¼ (GAT ìŠ¤í¬ë¦½íŠ¸ì™€ ê³µìœ  ê°€ëŠ¥)\n",
    "EMB_NPY  = \"node2vec_embeddings.npy\"  # ì €ì¥ë  ì„ë² ë”© (N,64)\n",
    "EDGE_PT  = \"edge_index.pt\"            # ì €ì¥ë  edge_index (CPU long)\n",
    "ADDR_CSV = \"addresses.csv\"            # ì €ì¥ë  ì£¼ì†Œ ìˆœì„œ\n",
    "N2V_PT   = \"node2vec.pt\"              # (ì„ íƒ) Node2Vec state_dict\n",
    "\n",
    "FORCE_RETRAIN = False  # Trueë©´ Node2Vecì„ ê°•ì œë¡œ ë‹¤ì‹œ í•™ìŠµ\n",
    "\n",
    "DEVICE = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "torch.manual_seed(42)\n",
    "np.random.seed(42)\n",
    "\n",
    "# ------------------------------------------------------------\n",
    "# 0) ë…¸ë“œ/ê·¸ë˜í”„ ë¡œë“œ\n",
    "if not os.path.exists(NODE_FEAT_CSV):\n",
    "    raise FileNotFoundError(f\"{NODE_FEAT_CSV} íŒŒì¼ì´ ì—†ìŠµë‹ˆë‹¤.\")\n",
    "if not os.path.exists(GRAPH_PKL):\n",
    "    raise FileNotFoundError(f\"{GRAPH_PKL} íŒŒì¼ì´ ì—†ìŠµë‹ˆë‹¤.\")\n",
    "\n",
    "node_df = pd.read_csv(NODE_FEAT_CSV)\n",
    "addresses_cur = node_df['address'].astype(str).tolist()\n",
    "addr2idx = {a: i for i, a in enumerate(addresses_cur)}\n",
    "\n",
    "with open(GRAPH_PKL, \"rb\") as f:\n",
    "    G_nx = pickle.load(f)  # networkx.MultiDiGraph\n",
    "\n",
    "# edge_index (CPU í…ì„œ)\n",
    "edges = []\n",
    "for u, v, _k in G_nx.edges(keys=True):\n",
    "    if u in addr2idx and v in addr2idx:\n",
    "        edges.append([addr2idx[u], addr2idx[v]])\n",
    "edge_index_cpu = torch.tensor(edges, dtype=torch.long).t().contiguous()  # [2,E] (CPU)\n",
    "\n",
    "num_nodes = len(addresses_cur)\n",
    "num_edges = edge_index_cpu.size(1)\n",
    "print(f\"DEVICE: {DEVICE} | nodes: {num_nodes} | edges: {num_edges}\")\n",
    "\n",
    "# (ì„ íƒ) ë¬´ë°©í–¥ íš¨ê³¼ ì¶”ê°€\n",
    "# edge_index_cpu = torch.cat([edge_index_cpu, edge_index_cpu.flip(0)], dim=1)\n",
    "\n",
    "# ------------------------------------------------------------\n",
    "# 1) Node2Vec ì„ë² ë”©: ì €ì¥ë³¸ì´ ìˆìœ¼ë©´ ë¡œë“œ, ì—†ìœ¼ë©´ í•™ìŠµ í›„ ì €ì¥\n",
    "use_saved = (not FORCE_RETRAIN) and os.path.exists(EMB_NPY) and os.path.exists(EDGE_PT) and os.path.exists(ADDR_CSV)\n",
    "\n",
    "def load_saved_embeddings():\n",
    "    addresses_saved = pd.read_csv(ADDR_CSV)['address'].astype(str).tolist()\n",
    "    if len(addresses_saved) != len(addresses_cur) or any(a!=b for a,b in zip(addresses_saved, addresses_cur)):\n",
    "        print(\"âš ï¸ ì €ì¥ëœ addresses.csv ì™€ í˜„ì¬ addresses ìˆœì„œ/ê°œìˆ˜ê°€ ë‹¤ë¦…ë‹ˆë‹¤. ì¬í•™ìŠµìœ¼ë¡œ ì „í™˜í•©ë‹ˆë‹¤.\")\n",
    "        return None, None, None\n",
    "\n",
    "    x_np = np.load(EMB_NPY)  # (N,64)\n",
    "    if x_np.shape != (len(addresses_cur), 64):\n",
    "        print(\"âš ï¸ ì €ì¥ëœ ì„ë² ë”© í¬ê¸°ê°€ ì˜ˆìƒê³¼ ë‹¤ë¦…ë‹ˆë‹¤. ì¬í•™ìŠµìœ¼ë¡œ ì „í™˜í•©ë‹ˆë‹¤.\")\n",
    "        return None, None, None\n",
    "\n",
    "    edge_idx_saved = torch.load(EDGE_PT, map_location=\"cpu\")\n",
    "    if edge_idx_saved.dtype != torch.long or edge_idx_saved.dim()!=2 or edge_idx_saved.size(0)!=2:\n",
    "        print(\"âš ï¸ ì €ì¥ëœ edge_index í˜•ì‹ì´ ì˜ˆìƒê³¼ ë‹¤ë¦…ë‹ˆë‹¤. ì¬í•™ìŠµìœ¼ë¡œ ì „í™˜í•©ë‹ˆë‹¤.\")\n",
    "        return None, None, None\n",
    "\n",
    "    print(\"âœ… ì €ì¥ëœ Node2Vec ì„ë² ë”©/ì—£ì§€/ì£¼ì†Œë¥¼ ë¶ˆëŸ¬ì˜µë‹ˆë‹¤.\")\n",
    "    return x_np, edge_idx_saved, addresses_saved\n",
    "\n",
    "x_init_cpu = None\n",
    "if use_saved:\n",
    "    x_np, edge_index_loaded, addresses_saved = load_saved_embeddings()\n",
    "    if x_np is not None:\n",
    "        x_init_cpu = torch.tensor(x_np, dtype=torch.float32)  # (N,64) CPU\n",
    "        edge_index_cpu = edge_index_loaded\n",
    "        addresses = addresses_saved\n",
    "    else:\n",
    "        use_saved = False\n",
    "\n",
    "if not use_saved:\n",
    "    print(\"ğŸ› ï¸ Node2Vec ì„ë² ë”©ì„ ìƒˆë¡œ í•™ìŠµí•©ë‹ˆë‹¤...\")\n",
    "    data_cpu = Data(edge_index=edge_index_cpu, num_nodes=num_nodes)\n",
    "\n",
    "    n2v = Node2Vec(\n",
    "        data_cpu.edge_index,\n",
    "        embedding_dim=64,\n",
    "        walk_length=30,\n",
    "        context_size=10,     # window\n",
    "        walks_per_node=200,  # num_walk\n",
    "        p=1.0, q=1.0,\n",
    "        num_negative_samples=1,\n",
    "        sparse=True\n",
    "    )\n",
    "    n2v_loader = n2v.loader(batch_size=128, shuffle=True, num_workers=0, persistent_workers=False)\n",
    "    n2v_optimizer = torch.optim.SparseAdam(list(n2v.parameters()), lr=0.01)\n",
    "\n",
    "    def train_node2vec(epochs=5):\n",
    "        n2v.train()\n",
    "        for epoch in range(1, epochs + 1):\n",
    "            total_loss = 0.0\n",
    "            for pos_rw, neg_rw in n2v_loader:\n",
    "                n2v_optimizer.zero_grad()\n",
    "                loss = n2v.loss(pos_rw, neg_rw)\n",
    "                loss.backward()\n",
    "                n2v_optimizer.step()\n",
    "                total_loss += loss.item()\n",
    "            print(f\"[Node2Vec] epoch {epoch:03d} | loss {total_loss/len(n2v_loader):.4f}\")\n",
    "\n",
    "    train_node2vec(epochs=5)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        x_init_cpu = n2v.embedding.weight.clone().detach()  # (N,64) CPU\n",
    "\n",
    "    # ì €ì¥\n",
    "    np.save(EMB_NPY, x_init_cpu.numpy())\n",
    "    torch.save(edge_index_cpu, EDGE_PT)\n",
    "    pd.Series(addresses_cur, name=\"address\").to_csv(ADDR_CSV, index=False)\n",
    "    try:\n",
    "        torch.save(n2v.state_dict(), N2V_PT)\n",
    "    except Exception as e:\n",
    "        print(f\"(ì°¸ê³ ) Node2Vec state ì €ì¥ ìƒëµ: {e}\")\n",
    "    addresses = addresses_cur\n",
    "    print(f\"âœ… ì €ì¥ ì™„ë£Œ: {EMB_NPY}, {EDGE_PT}, {ADDR_CSV}, {N2V_PT}\")\n",
    "\n",
    "# ìµœì¢… ì…ë ¥/ì—£ì§€ (DEVICEë¡œ ì´ë™)\n",
    "x_in = x_init_cpu.to(DEVICE)          # (N,64)\n",
    "edge_index_dev = edge_index_cpu.to(DEVICE)\n",
    "\n",
    "# ------------------------------------------------------------\n",
    "# 2) GraphSAGE ì˜¤í† ì¸ì½”ë” (DEVICEì—ì„œ ìˆ˜í–‰)\n",
    "class SAGEAutoEncoder(nn.Module):\n",
    "    def __init__(self, in_dim=64, dropout=0.3, aggr=\"mean\"):\n",
    "        super().__init__()\n",
    "        self.dropout = dropout\n",
    "        self.s1 = SAGEConv(in_dim, 256, aggr=aggr)  # 64 -> 256\n",
    "        self.s2 = SAGEConv(256, 32, aggr=aggr)      # 256 -> 32\n",
    "        self.s3 = SAGEConv(32, 64, aggr=aggr)       # 32 -> 64\n",
    "    def forward(self, x, edge_index):\n",
    "        x = F.dropout(x, p=self.dropout, training=self.training); x = F.elu(self.s1(x, edge_index))\n",
    "        x = F.dropout(x, p=self.dropout, training=self.training); x = F.elu(self.s2(x, edge_index))\n",
    "        x = F.dropout(x, p=self.dropout, training=self.training); x = self.s3(x, edge_index)\n",
    "        return x\n",
    "\n",
    "model = SAGEAutoEncoder(in_dim=64, dropout=0.3, aggr=\"mean\").to(DEVICE)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=1e-3, weight_decay=5e-4)\n",
    "\n",
    "# ------------------------------------------------------------\n",
    "# 3) í•™ìŠµ (1000 epochs, MSE ì¬êµ¬ì„±ì˜¤ì°¨)\n",
    "def train_sage_ae(epochs=1000):\n",
    "    model.train()\n",
    "    for epoch in range(1, epochs + 1):\n",
    "        optimizer.zero_grad()\n",
    "        x_hat = model(x_in, edge_index_dev)\n",
    "        loss = F.mse_loss(x_hat, x_in)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        if epoch % 50 == 0 or epoch == 1:\n",
    "            print(f\"[SAGE-AE] epoch {epoch:04d} | recon MSE {loss.item():.6f}\")\n",
    "\n",
    "train_sage_ae(epochs=1000)\n",
    "\n",
    "# ------------------------------------------------------------\n",
    "# 4) ì¬êµ¬ì„±ì˜¤ì°¨ â†’ z-score â†’ ìƒìœ„ 5% ì´ìƒì¹˜\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    x_recon = model(x_in, edge_index_dev)\n",
    "\n",
    "recon_err = torch.mean((x_recon - x_in) ** 2, dim=1).detach().cpu().numpy()\n",
    "mu = recon_err.mean()\n",
    "sigma = recon_err.std(ddof=1) if recon_err.size > 1 else 1e-8\n",
    "z = (recon_err - mu) / (sigma if sigma > 0 else 1e-8)\n",
    "z_cut = np.percentile(z, 95.0)\n",
    "anom = (z > z_cut).astype(int)\n",
    "\n",
    "print(f\"ìƒìœ„ 5% z-score ì„ê³„ê°’: {z_cut:.4f}\")\n",
    "print(f\"ì´ìƒì¹˜ ë…¸ë“œ ìˆ˜: {anom.sum()} / {len(anom)}\")\n",
    "\n",
    "# ------------------------------------------------------------\n",
    "# 5) ê²°ê³¼ ì €ì¥ (SAGE íŒŒì¼ëª…)\n",
    "out = pd.DataFrame({\n",
    "    \"address\": addresses,\n",
    "    \"recon_mse\": recon_err,\n",
    "    \"z_score\": z,\n",
    "    \"is_anomaly_top5pct\": anom\n",
    "})\n",
    "out.sort_values(\"z_score\", ascending=False).to_csv(\"sage_ae_anomalies.csv\", index=False, encoding=\"utf-8\")\n",
    "torch.save(model.state_dict(), \"sage_autoencoder.pt\")\n",
    "\n",
    "print(\"âœ… ì™„ë£Œ: sage_ae_anomalies.csv / sage_autoencoder.pt ìƒì„±\")\n",
    "print(f\"â„¹ï¸ ì„ë² ë”©/ì—£ì§€ëŠ” íŒŒì¼ë¡œ ë³´ê´€ë¨ â†’ {EMB_NPY}, {EDGE_PT}, {ADDR_CSV} (í•„ìš” ì‹œ ì¬ì‚¬ìš©)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8569784",
   "metadata": {},
   "source": [
    "### Node2vecìœ¼ë¡œ ê·¸ë˜í”„êµ¬ì¡°ì„ë² ë”©ê°’ê³¼ 29ê°œì˜ nodeí”¼ì³ë¥¼ ê²°í•©í•œ 64ì°¨ì›ì˜ ê·¸ë˜í”„ì„ë² ë”©ë²¡í„° ìƒì„±í›„ GAT,GraphSGAEì•Œê³ ë¦¬ì¦˜ í•™ìŠµ ì§„í–‰"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "1163f999",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DEVICE: cpu | nodes: 7958 | edges: 25601\n",
      "âœ… ì €ì¥ëœ Node2Vec ì„ë² ë”©/ì—£ì§€/ì£¼ì†Œ ì‚¬ìš©\n",
      "âœ… ê²°í•© ì„ë² ë”© ì €ì¥: combined_embeddings_64.npy\n",
      "[GAT_GAE] 0001 | loss 505.491791 | mse 108.071953 | bce 794.839661\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\user\\AppData\\Local\\Temp\\ipykernel_30980\\3738493665.py:87: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  edge_saved = torch.load(EDGE_PT, map_location=\"cpu\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[GAT_GAE] 0050 | loss 8.589891 | mse 2.541967 | bce 12.095848\n",
      "[GAT_GAE] 0100 | loss 3.123414 | mse 1.463450 | bce 3.319929\n",
      "[GAT_GAE] 0150 | loss 2.923254 | mse 1.345032 | bce 3.156443\n",
      "[GAT_GAE] 0200 | loss 2.850860 | mse 1.092607 | bce 3.516506\n",
      "[GAT_GAE] 0250 | loss 3.365918 | mse 1.056040 | bce 4.619755\n",
      "[GAT_GAE] 0300 | loss 2.799266 | mse 1.162821 | bce 3.272889\n",
      "[GAT_GAE] 0350 | loss 2.319847 | mse 1.054340 | bce 2.531014\n",
      "[GAT_GAE] 0400 | loss 2.057616 | mse 0.873196 | bce 2.368840\n",
      "[GAT_GAE] 0450 | loss 1.591910 | mse 0.789524 | bce 1.604770\n",
      "[GAT_GAE] 0500 | loss 1.530125 | mse 0.826689 | bce 1.406873\n",
      "[GAT_GAE] 0550 | loss 1.718401 | mse 0.818809 | bce 1.799184\n",
      "[GAT_GAE] 0600 | loss 1.560501 | mse 0.835535 | bce 1.449933\n",
      "[GAT_GAE] 0650 | loss 1.142637 | mse 0.689243 | bce 0.906788\n",
      "[GAT_GAE] 0700 | loss 1.374990 | mse 0.669408 | bce 1.411165\n",
      "[GAT_GAE] 0750 | loss 1.072892 | mse 0.648325 | bce 0.849134\n",
      "[GAT_GAE] 0800 | loss 0.996120 | mse 0.624989 | bce 0.742264\n",
      "[GAT_GAE] 0850 | loss 0.996045 | mse 0.628552 | bce 0.734985\n",
      "[GAT_GAE] 0900 | loss 1.202295 | mse 0.673077 | bce 1.058435\n",
      "[GAT_GAE] 0950 | loss 1.273426 | mse 0.780418 | bce 0.986016\n",
      "[GAT_GAE] 1000 | loss 1.091685 | mse 0.686920 | bce 0.809529\n",
      "âœ… GAT_GAE: ì €ì¥ ì™„ë£Œ â†’ gat_gae_anomalies.csv, gat_gae_model.pt\n",
      "[SAGE_GAE] 0001 | loss 255.322708 | mse 39.495514 | bce 431.654388\n",
      "[SAGE_GAE] 0050 | loss 8.867414 | mse 1.441756 | bce 14.851318\n",
      "[SAGE_GAE] 0100 | loss 8.019615 | mse 1.056638 | bce 13.925954\n",
      "[SAGE_GAE] 0150 | loss 5.502591 | mse 1.105912 | bce 8.793358\n",
      "[SAGE_GAE] 0200 | loss 6.488220 | mse 0.896375 | bce 11.183689\n",
      "[SAGE_GAE] 0250 | loss 6.650354 | mse 0.752015 | bce 11.796677\n",
      "[SAGE_GAE] 0300 | loss 5.775049 | mse 0.840772 | bce 9.868554\n",
      "[SAGE_GAE] 0350 | loss 5.320173 | mse 0.976250 | bce 8.687847\n",
      "[SAGE_GAE] 0400 | loss 5.236507 | mse 0.836794 | bce 8.799427\n",
      "[SAGE_GAE] 0450 | loss 5.533633 | mse 0.749355 | bce 9.568556\n",
      "[SAGE_GAE] 0500 | loss 5.488731 | mse 0.755069 | bce 9.467324\n",
      "[SAGE_GAE] 0550 | loss 5.415032 | mse 0.682198 | bce 9.465669\n",
      "[SAGE_GAE] 0600 | loss 4.633676 | mse 0.650568 | bce 7.966217\n",
      "[SAGE_GAE] 0650 | loss 5.511638 | mse 0.723205 | bce 9.576865\n",
      "[SAGE_GAE] 0700 | loss 4.521228 | mse 0.620664 | bce 7.801126\n",
      "[SAGE_GAE] 0750 | loss 4.645301 | mse 0.646136 | bce 7.998332\n",
      "[SAGE_GAE] 0800 | loss 3.461331 | mse 0.623467 | bce 5.675729\n",
      "[SAGE_GAE] 0850 | loss 4.506032 | mse 0.609955 | bce 7.792155\n",
      "[SAGE_GAE] 0900 | loss 4.702666 | mse 0.638586 | bce 8.128160\n",
      "[SAGE_GAE] 0950 | loss 3.855585 | mse 0.589831 | bce 6.531508\n",
      "[SAGE_GAE] 1000 | loss 3.825163 | mse 0.587440 | bce 6.475447\n",
      "âœ… SAGE_GAE: ì €ì¥ ì™„ë£Œ â†’ sage_gae_anomalies.csv, sage_gae_model.pt\n",
      "âœ… ì™„ë£Œ: combined_embeddings_64.npy / scaler.joblib / pca64.joblib ì €ì¥\n",
      "â„¹ï¸ GAT_GAE â†’ gat_gae_anomalies.csv, SAGE_GAE â†’ sage_gae_anomalies.csv\n"
     ]
    }
   ],
   "source": [
    "# -*- coding: utf-8 -*-\n",
    "\"\"\"\n",
    "Node2Vec(64, ì €ì¥/ì¬ì‚¬ìš©) + 29ì†ì„± ê²°í•© â†’ 64ì°¨ì› ì…ë ¥ ìƒì„±(PCA)\n",
    "â†’ GAT-GAE & SAGE-GAE ëª¨ë‘ í•™ìŠµ\n",
    "- íŠ¹ì§• ì¬êµ¬ì„±(MSE) + ë§í¬ ì¬êµ¬ì„±(BCE) ë™ì‹œ í•™ìŠµ\n",
    "\"\"\"\n",
    "\n",
    "import os\n",
    "import pickle\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "from torch import nn\n",
    "import torch.nn.functional as F\n",
    "from torch_geometric.nn import Node2Vec, GATConv, SAGEConv\n",
    "from torch_geometric.data import Data\n",
    "from torch_geometric.utils import negative_sampling\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.decomposition import PCA\n",
    "from joblib import dump, load\n",
    "\n",
    "# ----------------------------- ì„¤ì • ------------------------------\n",
    "NODE_FEAT_CSV = \"node_features.csv\"\n",
    "GRAPH_PKL     = \"G_base_multidigraph.pkl\"\n",
    "\n",
    "# Node2Vec ì‚°ì¶œë¬¼\n",
    "EMB_NPY  = \"node2vec_embeddings.npy\"\n",
    "EDGE_PT  = \"edge_index.pt\"\n",
    "ADDR_CSV = \"addresses.csv\"\n",
    "N2V_PT   = \"node2vec.pt\"\n",
    "\n",
    "# ê²°í•© ì„ë² ë”©/ì „ì²˜ë¦¬ ì €ì¥\n",
    "COMB_EMB_NPY = \"combined_embeddings_64.npy\"\n",
    "SCALER_P     = \"scaler.joblib\"\n",
    "PCA64_P      = \"pca64.joblib\"\n",
    "\n",
    "FORCE_RETRAIN_N2V = False      # Trueë©´ Node2Vec ì¬í•™ìŠµ ê°•ì œ\n",
    "LAMBDA_BCE        = 0.5        # ì´ì†ì‹¤ = MSE + Î»*BCE\n",
    "EPOCHS            = 1000\n",
    "DROPOUT           = 0.3\n",
    "LR                = 1e-3\n",
    "WEIGHT_DECAY      = 5e-4\n",
    "SEED              = 42\n",
    "\n",
    "DEVICE = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "torch.manual_seed(SEED); np.random.seed(SEED)\n",
    "\n",
    "# ---------------------- 0) ì£¼ì†Œ/ê·¸ë˜í”„ ë¡œë“œ -----------------------\n",
    "if not os.path.exists(NODE_FEAT_CSV):\n",
    "    raise FileNotFoundError(f\"{NODE_FEAT_CSV} ì—†ìŒ\")\n",
    "if not os.path.exists(GRAPH_PKL):\n",
    "    raise FileNotFoundError(f\"{GRAPH_PKL} ì—†ìŒ\")\n",
    "\n",
    "node_df = pd.read_csv(NODE_FEAT_CSV)\n",
    "addresses_cur = node_df['address'].astype(str).tolist()\n",
    "addr2idx = {a: i for i,a in enumerate(addresses_cur)}\n",
    "\n",
    "with open(GRAPH_PKL, \"rb\") as f:\n",
    "    import networkx as nx\n",
    "    G_nx = pickle.load(f)  # MultiDiGraph\n",
    "\n",
    "# edge_index (CPU)\n",
    "edges = []\n",
    "for u, v, _k in G_nx.edges(keys=True):\n",
    "    if u in addr2idx and v in addr2idx:\n",
    "        edges.append([addr2idx[u], addr2idx[v]])\n",
    "edge_index_cpu = torch.tensor(edges, dtype=torch.long).t().contiguous()\n",
    "\n",
    "num_nodes = len(addresses_cur)\n",
    "print(f\"DEVICE: {DEVICE} | nodes: {num_nodes} | edges: {edge_index_cpu.size(1)}\")\n",
    "\n",
    "# (ì„ íƒ) ì–‘ë°©í–¥ ì¶”ê°€\n",
    "# edge_index_cpu = torch.cat([edge_index_cpu, edge_index_cpu.flip(0)], dim=1)\n",
    "\n",
    "# ---------------------- 1) Node2Vec ë¡œë“œ/í•™ìŠµ ----------------------\n",
    "def try_load_saved_n2v():\n",
    "    if not (os.path.exists(EMB_NPY) and os.path.exists(EDGE_PT) and os.path.exists(ADDR_CSV)):\n",
    "        return None\n",
    "    addr_saved = pd.read_csv(ADDR_CSV)['address'].astype(str).tolist()\n",
    "    if len(addr_saved)!=len(addresses_cur) or any(a!=b for a,b in zip(addr_saved, addresses_cur)):\n",
    "        print(\"âš ï¸ ì €ì¥ëœ addressesì™€ í˜„ì¬ê°€ ë¶ˆì¼ì¹˜ â†’ N2V ì¬í•™ìŠµ\")\n",
    "        return None\n",
    "    x_np = np.load(EMB_NPY)\n",
    "    if x_np.shape!=(len(addresses_cur), 64):\n",
    "        print(\"âš ï¸ ì €ì¥ëœ ì„ë² ë”© í¬ê¸° ë¶ˆì¼ì¹˜ â†’ N2V ì¬í•™ìŠµ\")\n",
    "        return None\n",
    "    edge_saved = torch.load(EDGE_PT, map_location=\"cpu\")\n",
    "    if edge_saved.dtype!=torch.long or edge_saved.dim()!=2 or edge_saved.size(0)!=2:\n",
    "        print(\"âš ï¸ ì €ì¥ëœ edge_index í˜•ì‹ ì´ìƒ â†’ N2V ì¬í•™ìŠµ\")\n",
    "        return None\n",
    "    print(\"âœ… ì €ì¥ëœ Node2Vec ì„ë² ë”©/ì—£ì§€/ì£¼ì†Œ ì‚¬ìš©\")\n",
    "    return torch.tensor(x_np, dtype=torch.float32), edge_saved, addr_saved\n",
    "\n",
    "x_n2v_cpu = None\n",
    "if not FORCE_RETRAIN_N2V:\n",
    "    loaded = try_load_saved_n2v()\n",
    "    if loaded:\n",
    "        x_n2v_cpu, edge_index_cpu, addresses = loaded\n",
    "\n",
    "if x_n2v_cpu is None:\n",
    "    print(\"ğŸ› ï¸ Node2Vecë¥¼ ìƒˆë¡œ í•™ìŠµí•©ë‹ˆë‹¤...\")\n",
    "    data_cpu = Data(edge_index=edge_index_cpu, num_nodes=num_nodes)\n",
    "    n2v = Node2Vec(\n",
    "        data_cpu.edge_index, embedding_dim=64,\n",
    "        walk_length=30, context_size=10, walks_per_node=200,\n",
    "        p=1.0, q=1.0, num_negative_samples=1, sparse=True\n",
    "    )\n",
    "    n2v_loader = n2v.loader(batch_size=128, shuffle=True, num_workers=0, persistent_workers=False)\n",
    "    n2v_opt = torch.optim.SparseAdam(list(n2v.parameters()), lr=0.01)\n",
    "\n",
    "    def train_n2v(epochs=5):\n",
    "        n2v.train()\n",
    "        for ep in range(1, epochs+1):\n",
    "            tot=0.0\n",
    "            for pos_rw, neg_rw in n2v_loader:\n",
    "                n2v_opt.zero_grad()\n",
    "                loss = n2v.loss(pos_rw, neg_rw)\n",
    "                loss.backward(); n2v_opt.step()\n",
    "                tot += loss.item()\n",
    "            print(f\"[Node2Vec] epoch {ep:03d} | loss {tot/len(n2v_loader):.4f}\")\n",
    "\n",
    "    train_n2v(epochs=5)\n",
    "    with torch.no_grad():\n",
    "        x_n2v_cpu = n2v.embedding.weight.clone().detach()   # (N,64)\n",
    "    # ì €ì¥\n",
    "    np.save(EMB_NPY, x_n2v_cpu.numpy())\n",
    "    torch.save(edge_index_cpu, EDGE_PT)\n",
    "    pd.Series(addresses_cur, name=\"address\").to_csv(ADDR_CSV, index=False)\n",
    "    try: torch.save(n2v.state_dict(), N2V_PT)\n",
    "    except Exception as e: print(f\"(ì°¸ê³ ) Node2Vec state ì €ì¥ ìƒëµ: {e}\")\n",
    "    addresses = addresses_cur\n",
    "    print(\"âœ… Node2Vec ì €ì¥ ì™„ë£Œ\")\n",
    "\n",
    "# ---------------- 2) 29ì†ì„±ê³¼ ê²°í•© â†’ 64ì°¨ íˆ¬ì˜(PCA) ----------------\n",
    "node = pd.read_csv(NODE_FEAT_CSV).set_index(\"address\")\n",
    "feat_cols = [c for c in node.columns]  # ì£¼ì†Œ ì œì™¸ 29ê°œ\n",
    "X_attr = node.reindex(addresses)[feat_cols].replace([np.inf,-np.inf], np.nan).fillna(0).astype(float).values  # (N,29)\n",
    "\n",
    "# í‘œì¤€í™”\n",
    "if os.path.exists(SCALER_P) and os.path.exists(PCA64_P):\n",
    "    try:\n",
    "        scaler = load(SCALER_P); pca64 = load(PCA64_P)\n",
    "        print(\"âœ… ì €ì¥ëœ Scaler/PCA ë¡œë“œ\")\n",
    "    except Exception:\n",
    "        scaler = StandardScaler().fit(X_attr)\n",
    "        pca64  = PCA(n_components=64, random_state=SEED).fit(np.hstack([x_n2v_cpu.numpy(), scaler.transform(X_attr)]))\n",
    "        dump(scaler, SCALER_P); dump(pca64, PCA64_P)\n",
    "else:\n",
    "    scaler = StandardScaler().fit(X_attr)\n",
    "    dump(scaler, SCALER_P)\n",
    "    # concat(64+29=93) â†’ PCA 64\n",
    "    X_concat_fit = np.hstack([x_n2v_cpu.numpy(), scaler.transform(X_attr)])\n",
    "    pca64 = PCA(n_components=64, random_state=SEED).fit(X_concat_fit)\n",
    "    dump(pca64, PCA64_P)\n",
    "\n",
    "X_concat = np.hstack([x_n2v_cpu.numpy(), scaler.transform(X_attr)])   # (N,93)\n",
    "X64 = pca64.transform(X_concat).astype(np.float32)                     # (N,64)\n",
    "np.save(COMB_EMB_NPY, X64)\n",
    "print(\"âœ… ê²°í•© ì„ë² ë”© ì €ì¥:\", COMB_EMB_NPY)\n",
    "\n",
    "x_in = torch.tensor(X64, dtype=torch.float32, device=DEVICE)\n",
    "edge_index_dev = edge_index_cpu.to(DEVICE)\n",
    "\n",
    "# ----------------------- 3) ëª¨ë¸ ì •ì˜ ------------------------\n",
    "class GAT_GAE(nn.Module):\n",
    "    def __init__(self, in_dim=64, dropout=0.3):\n",
    "        super().__init__()\n",
    "        self.do = dropout\n",
    "        self.g1 = GATConv(in_dim, 32, heads=8, concat=True, dropout=dropout)  # 64 -> 256\n",
    "        self.g2 = GATConv(256, 32, heads=1, concat=True, dropout=dropout)     # 256 -> 32 (z)\n",
    "        self.g3 = GATConv(32, 64, heads=1, concat=True, dropout=dropout)      # 32 -> 64 (x_hat)\n",
    "    def forward(self, x, ei):\n",
    "        x = F.dropout(x, p=self.do, training=self.training); x = F.elu(self.g1(x, ei))\n",
    "        x = F.dropout(x, p=self.do, training=self.training); z = F.elu(self.g2(x, ei))\n",
    "        x_hat = F.dropout(z, p=self.do, training=self.training); x_hat = self.g3(x_hat, ei)\n",
    "        return x_hat, z\n",
    "\n",
    "class SAGE_GAE(nn.Module):\n",
    "    def __init__(self, in_dim=64, dropout=0.3, aggr=\"mean\"):\n",
    "        super().__init__()\n",
    "        self.do = dropout\n",
    "        self.s1 = SAGEConv(in_dim, 256, aggr=aggr)\n",
    "        self.s2 = SAGEConv(256, 32, aggr=aggr)   # z\n",
    "        self.s3 = SAGEConv(32, 64, aggr=aggr)    # x_hat\n",
    "    def forward(self, x, ei):\n",
    "        x = F.dropout(x, p=self.do, training=self.training); x = F.elu(self.s1(x, ei))\n",
    "        z = F.dropout(x, p=self.do, training=self.training); z = F.elu(self.s2(z, ei))\n",
    "        x_hat = F.dropout(z, p=self.do, training=self.training); x_hat = self.s3(x_hat, ei)\n",
    "        return x_hat, z\n",
    "\n",
    "def link_bce_loss(z, edge_index, num_neg=None):\n",
    "    # pos/neg ìƒ˜í”Œ\n",
    "    pos = edge_index\n",
    "    if num_neg is None: num_neg = pos.size(1)\n",
    "    neg = negative_sampling(pos, num_nodes=z.size(0), num_neg_samples=num_neg, method='sparse')\n",
    "    # dot decode\n",
    "    def dot_decode(z, e): return (z[e[0]] * z[e[1]]).sum(dim=1)\n",
    "    logits = torch.cat([dot_decode(z, pos), dot_decode(z, neg)], dim=0)\n",
    "    labels = torch.cat([\n",
    "        torch.ones(pos.size(1), device=logits.device),\n",
    "        torch.zeros(neg.size(1), device=logits.device)\n",
    "    ], dim=0)\n",
    "    return F.binary_cross_entropy_with_logits(logits, labels)\n",
    "\n",
    "def train_gae(model, epochs, name):\n",
    "    model = model.to(DEVICE)\n",
    "    opt = torch.optim.Adam(model.parameters(), lr=LR, weight_decay=WEIGHT_DECAY)\n",
    "    for ep in range(1, epochs+1):\n",
    "        model.train(); opt.zero_grad()\n",
    "        x_hat, z = model(x_in, edge_index_dev)\n",
    "        mse = F.mse_loss(x_hat, x_in)\n",
    "        bce = link_bce_loss(z, edge_index_dev)\n",
    "        loss = mse + LAMBDA_BCE * bce\n",
    "        loss.backward(); opt.step()\n",
    "        if ep==1 or ep%50==0:\n",
    "            print(f\"[{name}] {ep:04d} | loss {loss.item():.6f} | mse {mse.item():.6f} | bce {bce.item():.6f}\")\n",
    "    # í‰ê°€/ì €ì¥\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        x_hat, z = model(x_in, edge_index_dev)\n",
    "    recon = torch.mean((x_hat - x_in)**2, dim=1).detach().cpu().numpy()\n",
    "    mu, sd = recon.mean(), recon.std(ddof=1) if recon.size>1 else 1e-8\n",
    "    zscore = (recon - mu) / (sd if sd>0 else 1e-8)\n",
    "    cut = np.percentile(zscore, 95.0)\n",
    "    anom = (zscore > cut).astype(int)\n",
    "\n",
    "    out = pd.DataFrame({\"address\": addresses, \"recon_mse\": recon, \"z_score\": zscore, \"is_anomaly_top5pct\": anom})\n",
    "    csv_name = f\"{name.lower()}_anomalies.csv\"\n",
    "    pt_name  = f\"{name.lower()}_model.pt\"\n",
    "    out.sort_values(\"z_score\", ascending=False).to_csv(csv_name, index=False, encoding=\"utf-8\")\n",
    "    torch.save(model.state_dict(), pt_name)\n",
    "    print(f\"âœ… {name}: ì €ì¥ ì™„ë£Œ â†’ {csv_name}, {pt_name}\")\n",
    "    return out\n",
    "\n",
    "# -------------------- 4) ë‘ ëª¨ë¸ ì—°ì† ì‹¤í–‰ --------------------\n",
    "gat_out  = train_gae(GAT_GAE(in_dim=64, dropout=DROPOUT),  EPOCHS, \"GAT_GAE\")\n",
    "sage_out = train_gae(SAGE_GAE(in_dim=64, dropout=DROPOUT), EPOCHS, \"SAGE_GAE\")\n",
    "\n",
    "print(\"âœ… ì™„ë£Œ: combined_embeddings_64.npy / scaler.joblib / pca64.joblib ì €ì¥\")\n",
    "print(\"â„¹ï¸ GAT_GAE â†’ gat_gae_anomalies.csv, SAGE_GAE â†’ sage_gae_anomalies.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e972d63b",
   "metadata": {},
   "source": [
    "### GAT,GraphSAGE ëª¨ë‘ ë¯¸ë‹ˆë°°ì¹˜ë°©ì‹ìœ¼ë¡œ í•™ìŠµí•œ ë°©ì‹"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f5d3b754",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DEVICE: cpu | nodes: 7958 | edges: 25601\n",
      "âœ… ì €ì¥ëœ Node2Vec ì„ë² ë”©/ì—£ì§€/ì£¼ì†Œ ì‚¬ìš©\n",
      "âœ… ì €ì¥ëœ Scaler/PCA ë¡œë“œ\n",
      "âœ… ê²°í•© ì„ë² ë”© ì €ì¥: combined_embeddings_64.npy\n",
      "[GAT_GAE] 0001 | loss 242.304894 | mse 71.805924 | bce 340.997938\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\user\\AppData\\Local\\Temp\\ipykernel_30980\\3282253683.py:92: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  edge_saved = torch.load(EDGE_PT, map_location=\"cpu\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[GAT_GAE] 0050 | loss 3.195619 | mse 1.206307 | bce 3.978624\n",
      "[GAT_GAE] 0100 | loss 1.972099 | mse 0.952752 | bce 2.038694\n",
      "[GAT_GAE] 0150 | loss 1.109245 | mse 0.702397 | bce 0.813695\n",
      "[GAT_GAE] 0200 | loss 1.006012 | mse 0.599225 | bce 0.813574\n",
      "[GAT_GAE] 0250 | loss 0.981288 | mse 0.597751 | bce 0.767074\n",
      "[GAT_GAE] 0300 | loss 0.906257 | mse 0.566926 | bce 0.678661\n",
      "[GAT_GAE] 0350 | loss 1.046467 | mse 0.609092 | bce 0.874750\n",
      "[GAT_GAE] 0400 | loss 0.929497 | mse 0.557273 | bce 0.744447\n",
      "[GAT_GAE] 0450 | loss 0.822681 | mse 0.518042 | bce 0.609278\n",
      "[GAT_GAE] 0500 | loss 0.849261 | mse 0.536495 | bce 0.625531\n",
      "[GAT_GAE] 0550 | loss 0.861889 | mse 0.521575 | bce 0.680628\n",
      "[GAT_GAE] 0600 | loss 0.815565 | mse 0.507944 | bce 0.615243\n",
      "[GAT_GAE] 0650 | loss 0.958153 | mse 0.503514 | bce 0.909279\n",
      "[GAT_GAE] 0700 | loss 0.803176 | mse 0.501523 | bce 0.603306\n",
      "[GAT_GAE] 0750 | loss 0.752475 | mse 0.491933 | bce 0.521083\n",
      "[GAT_GAE] 0800 | loss 0.788474 | mse 0.473126 | bce 0.630697\n",
      "[GAT_GAE] 0850 | loss 0.777943 | mse 0.475340 | bce 0.605208\n",
      "[GAT_GAE] 0900 | loss 0.805074 | mse 0.484082 | bce 0.641983\n",
      "[GAT_GAE] 0950 | loss 0.816129 | mse 0.506631 | bce 0.618996\n",
      "[GAT_GAE] 1000 | loss 0.830228 | mse 0.505723 | bce 0.649010\n",
      "âœ… GAT_GAE (mini-batch): ì €ì¥ ì™„ë£Œ â†’ gat_gae_mb_anomalies.csv, gat_gae_mb_model.pt\n",
      "[SAGE_GAE] 0001 | loss 103.659994 | mse 15.968233 | bce 175.383526\n",
      "[SAGE_GAE] 0050 | loss 6.287239 | mse 0.846734 | bce 10.881010\n",
      "[SAGE_GAE] 0100 | loss 5.351128 | mse 0.612473 | bce 9.477309\n",
      "[SAGE_GAE] 0150 | loss 4.532990 | mse 0.556760 | bce 7.952461\n",
      "[SAGE_GAE] 0200 | loss 3.936536 | mse 0.515066 | bce 6.842940\n",
      "[SAGE_GAE] 0250 | loss 2.777448 | mse 0.487692 | bce 4.579512\n",
      "[SAGE_GAE] 0300 | loss 2.548775 | mse 0.488837 | bce 4.119875\n",
      "[SAGE_GAE] 0350 | loss 1.542613 | mse 0.462057 | bce 2.161113\n",
      "[SAGE_GAE] 0400 | loss 1.283896 | mse 0.455370 | bce 1.657052\n",
      "[SAGE_GAE] 0450 | loss 1.098740 | mse 0.453026 | bce 1.291429\n",
      "[SAGE_GAE] 0500 | loss 0.940331 | mse 0.433482 | bce 1.013699\n",
      "[SAGE_GAE] 0550 | loss 0.811859 | mse 0.416819 | bce 0.790080\n",
      "[SAGE_GAE] 0600 | loss 0.696285 | mse 0.389331 | bce 0.613907\n",
      "[SAGE_GAE] 0650 | loss 0.758409 | mse 0.393461 | bce 0.729895\n",
      "[SAGE_GAE] 0700 | loss 0.864901 | mse 0.407849 | bce 0.914104\n",
      "[SAGE_GAE] 0750 | loss 0.756307 | mse 0.403195 | bce 0.706225\n",
      "[SAGE_GAE] 0800 | loss 0.808774 | mse 0.391013 | bce 0.835523\n",
      "[SAGE_GAE] 0850 | loss 0.796843 | mse 0.433482 | bce 0.726723\n",
      "[SAGE_GAE] 0900 | loss 0.846595 | mse 0.395303 | bce 0.902584\n",
      "[SAGE_GAE] 0950 | loss 0.802247 | mse 0.404988 | bce 0.794518\n",
      "[SAGE_GAE] 1000 | loss 0.807942 | mse 0.394918 | bce 0.826047\n",
      "âœ… SAGE_GAE (mini-batch): ì €ì¥ ì™„ë£Œ â†’ sage_gae_mb_anomalies.csv, sage_gae_mb_model.pt\n",
      "âœ… ì™„ë£Œ: combined_embeddings_64.npy / scaler.joblib / pca64.joblib ì €ì¥\n",
      "â„¹ï¸ GAT_GAE â†’ gat_gae_mb_anomalies.csv, SAGE_GAE â†’ sage_gae_mb_anomalies.csv\n"
     ]
    }
   ],
   "source": [
    "# -*- coding: utf-8 -*-\n",
    "\"\"\"\n",
    "ë¯¸ë‹ˆë°°ì¹˜(Neighbor Sampling)ë¡œ í•™ìŠµí•˜ëŠ” GAT-GAE & SAGE-GAE\n",
    "ì…ë ¥: Node2Vec(64) + 29ì†ì„± ê²°í•© â†’ PCA 64 (ì €ì¥/ì¬ì‚¬ìš©)\n",
    "ì†ì‹¤: íŠ¹ì§• ì¬êµ¬ì„±(MSE) + ë§í¬ ì¬êµ¬ì„±(BCE, seed ë‚´ë¶€ ë§í¬)\n",
    "í‰ê°€ë„ ë¯¸ë‹ˆë°°ì¹˜ë¡œ seedë³„ x_hatì„ ë¶™ì—¬ ì „ì²´ recon MSE ê³„ì‚°\n",
    "\"\"\"\n",
    "\n",
    "import os, pickle\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "from torch import nn\n",
    "import torch.nn.functional as F\n",
    "from torch_geometric.nn import Node2Vec, GATConv, SAGEConv\n",
    "from torch_geometric.data import Data\n",
    "from torch_geometric.utils import negative_sampling\n",
    "from torch_geometric.loader import NeighborLoader\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.decomposition import PCA\n",
    "from joblib import dump, load\n",
    "\n",
    "# ----------------------------- ì„¤ì • ------------------------------\n",
    "NODE_FEAT_CSV = \"node_features.csv\"\n",
    "GRAPH_PKL     = \"G_base_multidigraph.pkl\"\n",
    "\n",
    "# Node2Vec ì‚°ì¶œë¬¼\n",
    "EMB_NPY  = \"node2vec_embeddings.npy\"\n",
    "EDGE_PT  = \"edge_index.pt\"\n",
    "ADDR_CSV = \"addresses.csv\"\n",
    "N2V_PT   = \"node2vec.pt\"\n",
    "\n",
    "# ê²°í•© ì„ë² ë”©/ì „ì²˜ë¦¬ ì €ì¥\n",
    "COMB_EMB_NPY = \"combined_embeddings_64.npy\"\n",
    "SCALER_P     = \"scaler.joblib\"\n",
    "PCA64_P      = \"pca64.joblib\"\n",
    "\n",
    "# ë¯¸ë‹ˆë°°ì¹˜ í•˜ì´í¼íŒŒë¼ë¯¸í„°\n",
    "BATCH_SIZE   = 1024        # ë…¸ë“œ seed ë°°ì¹˜ í¬ê¸°\n",
    "FANOUTS      = [15, 10, 5] # ë ˆì´ì–´ë³„ ì´ì›ƒ ìƒ˜í”Œ í¬ê¸°(=num_neighbors)\n",
    "NUM_WORKERS  = 0           # Windows/Jupyter ì•ˆì „ê°’\n",
    "PERSISTENT   = False\n",
    "\n",
    "FORCE_RETRAIN_N2V = False\n",
    "LAMBDA_BCE        = 0.5\n",
    "EPOCHS            = 1000\n",
    "DROPOUT           = 0.3\n",
    "LR                = 1e-3\n",
    "WEIGHT_DECAY      = 5e-4\n",
    "SEED              = 42\n",
    "\n",
    "DEVICE = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "torch.manual_seed(SEED); np.random.seed(SEED)\n",
    "\n",
    "# ---------------------- 0) ì£¼ì†Œ/ê·¸ë˜í”„ ë¡œë“œ -----------------------\n",
    "if not os.path.exists(NODE_FEAT_CSV):\n",
    "    raise FileNotFoundError(f\"{NODE_FEAT_CSV} ì—†ìŒ\")\n",
    "if not os.path.exists(GRAPH_PKL):\n",
    "    raise FileNotFoundError(f\"{GRAPH_PKL} ì—†ìŒ\")\n",
    "\n",
    "node_df = pd.read_csv(NODE_FEAT_CSV)\n",
    "addresses_cur = node_df['address'].astype(str).tolist()\n",
    "addr2idx = {a: i for i,a in enumerate(addresses_cur)}\n",
    "\n",
    "with open(GRAPH_PKL, \"rb\") as f:\n",
    "    import networkx as nx\n",
    "    G_nx = pickle.load(f)  # MultiDiGraph\n",
    "\n",
    "edges = []\n",
    "for u, v, _k in G_nx.edges(keys=True):\n",
    "    if u in addr2idx and v in addr2idx:\n",
    "        edges.append([addr2idx[u], addr2idx[v]])\n",
    "edge_index_cpu = torch.tensor(edges, dtype=torch.long).t().contiguous()  # [2,E] CPU\n",
    "num_nodes = len(addresses_cur)\n",
    "print(f\"DEVICE: {DEVICE} | nodes: {num_nodes} | edges: {edge_index_cpu.size(1)}\")\n",
    "\n",
    "# (ì„ íƒ) ì–‘ë°©í–¥ ì¶”ê°€: ì•ˆì •ì„±â†‘\n",
    "# edge_index_cpu = torch.cat([edge_index_cpu, edge_index_cpu.flip(0)], dim=1)\n",
    "\n",
    "# ---------------------- 1) Node2Vec ë¡œë“œ/í•™ìŠµ ----------------------\n",
    "def try_load_saved_n2v():\n",
    "    if not (os.path.exists(EMB_NPY) and os.path.exists(EDGE_PT) and os.path.exists(ADDR_CSV)):\n",
    "        return None\n",
    "    addr_saved = pd.read_csv(ADDR_CSV)['address'].astype(str).tolist()\n",
    "    if len(addr_saved)!=len(addresses_cur) or any(a!=b for a,b in zip(addr_saved, addresses_cur)):\n",
    "        print(\"âš ï¸ ì €ì¥ëœ addressesì™€ í˜„ì¬ê°€ ë¶ˆì¼ì¹˜ â†’ N2V ì¬í•™ìŠµ\")\n",
    "        return None\n",
    "    x_np = np.load(EMB_NPY)\n",
    "    if x_np.shape!=(len(addresses_cur), 64):\n",
    "        print(\"âš ï¸ ì €ì¥ëœ ì„ë² ë”© í¬ê¸° ë¶ˆì¼ì¹˜ â†’ N2V ì¬í•™ìŠµ\")\n",
    "        return None\n",
    "    edge_saved = torch.load(EDGE_PT, map_location=\"cpu\")\n",
    "    if edge_saved.dtype!=torch.long or edge_saved.dim()!=2 or edge_saved.size(0)!=2:\n",
    "        print(\"âš ï¸ ì €ì¥ëœ edge_index í˜•ì‹ ì´ìƒ â†’ N2V ì¬í•™ìŠµ\")\n",
    "        return None\n",
    "    print(\"âœ… ì €ì¥ëœ Node2Vec ì„ë² ë”©/ì—£ì§€/ì£¼ì†Œ ì‚¬ìš©\")\n",
    "    return torch.tensor(x_np, dtype=torch.float32), edge_saved, addr_saved\n",
    "\n",
    "x_n2v_cpu = None\n",
    "if not FORCE_RETRAIN_N2V:\n",
    "    loaded = try_load_saved_n2v()\n",
    "    if loaded:\n",
    "        x_n2v_cpu, edge_index_cpu, addresses = loaded\n",
    "\n",
    "if x_n2v_cpu is None:\n",
    "    print(\"ğŸ› ï¸ Node2Vecë¥¼ ìƒˆë¡œ í•™ìŠµí•©ë‹ˆë‹¤...\")\n",
    "    data_cpu = Data(edge_index=edge_index_cpu, num_nodes=num_nodes)\n",
    "    n2v = Node2Vec(\n",
    "        data_cpu.edge_index, embedding_dim=64,\n",
    "        walk_length=30, context_size=10, walks_per_node=200,\n",
    "        p=1.0, q=1.0, num_negative_samples=1, sparse=True\n",
    "    )\n",
    "    n2v_loader = n2v.loader(batch_size=128, shuffle=True, num_workers=0, persistent_workers=False)\n",
    "    n2v_opt = torch.optim.SparseAdam(list(n2v.parameters()), lr=0.01)\n",
    "    def train_n2v(epochs=5):\n",
    "        n2v.train()\n",
    "        for ep in range(1, epochs+1):\n",
    "            tot=0.0\n",
    "            for pos_rw, neg_rw in n2v_loader:\n",
    "                n2v_opt.zero_grad()\n",
    "                loss = n2v.loss(pos_rw, neg_rw)\n",
    "                loss.backward(); n2v_opt.step()\n",
    "                tot += loss.item()\n",
    "            print(f\"[Node2Vec] epoch {ep:03d} | loss {tot/len(n2v_loader):.4f}\")\n",
    "    train_n2v(epochs=5)\n",
    "    with torch.no_grad():\n",
    "        x_n2v_cpu = n2v.embedding.weight.clone().detach()   # (N,64)\n",
    "    # ì €ì¥\n",
    "    np.save(EMB_NPY, x_n2v_cpu.numpy())\n",
    "    torch.save(edge_index_cpu, EDGE_PT)\n",
    "    pd.Series(addresses_cur, name=\"address\").to_csv(ADDR_CSV, index=False)\n",
    "    try: torch.save(n2v.state_dict(), N2V_PT)\n",
    "    except Exception as e: print(f\"(ì°¸ê³ ) Node2Vec state ì €ì¥ ìƒëµ: {e}\")\n",
    "    addresses = addresses_cur\n",
    "    print(\"âœ… Node2Vec ì €ì¥ ì™„ë£Œ\")\n",
    "\n",
    "# ---------------- 2) 29ì†ì„±ê³¼ ê²°í•© â†’ 64ì°¨ íˆ¬ì˜(PCA) ----------------\n",
    "node = pd.read_csv(NODE_FEAT_CSV).set_index(\"address\")\n",
    "feat_cols = [c for c in node.columns]\n",
    "X_attr = node.reindex(addresses)[feat_cols].replace([np.inf,-np.inf], np.nan).fillna(0).astype(float).values  # (N,29)\n",
    "\n",
    "if os.path.exists(SCALER_P) and os.path.exists(PCA64_P):\n",
    "    try:\n",
    "        scaler = load(SCALER_P); pca64 = load(PCA64_P)\n",
    "        print(\"âœ… ì €ì¥ëœ Scaler/PCA ë¡œë“œ\")\n",
    "    except Exception:\n",
    "        scaler = StandardScaler().fit(X_attr)\n",
    "        X_concat_fit = np.hstack([x_n2v_cpu.numpy(), scaler.transform(X_attr)])\n",
    "        pca64  = PCA(n_components=64, random_state=SEED).fit(X_concat_fit)\n",
    "        dump(scaler, SCALER_P); dump(pca64, PCA64_P)\n",
    "else:\n",
    "    scaler = StandardScaler().fit(X_attr)\n",
    "    X_concat_fit = np.hstack([x_n2v_cpu.numpy(), scaler.transform(X_attr)])\n",
    "    pca64  = PCA(n_components=64, random_state=SEED).fit(X_concat_fit)\n",
    "    dump(scaler, SCALER_P); dump(pca64, PCA64_P)\n",
    "\n",
    "X_concat = np.hstack([x_n2v_cpu.numpy(), scaler.transform(X_attr)])   # (N,93)\n",
    "X64 = pca64.transform(X_concat).astype(np.float32)                     # (N,64)\n",
    "np.save(COMB_EMB_NPY, X64)\n",
    "print(\"âœ… ê²°í•© ì„ë² ë”© ì €ì¥:\", COMB_EMB_NPY)\n",
    "\n",
    "x_all = torch.tensor(X64, dtype=torch.float32, device=DEVICE)  # ì „ì²´ ë…¸ë“œ íŠ¹ì„±(64)\n",
    "data_full = Data(edge_index=edge_index_cpu, num_nodes=num_nodes)  # íŠ¹ì§•ì€ ì™¸ë¶€ x_all ì‚¬ìš©\n",
    "\n",
    "# ----------------------- 3) ë¯¸ë‹ˆë°°ì¹˜ ë¡œë” --------------------------\n",
    "train_loader = NeighborLoader(\n",
    "    data_full,\n",
    "    input_nodes=torch.arange(num_nodes),      # ì „ì²´ ë…¸ë“œ ëŒ€ìƒ\n",
    "    num_neighbors=FANOUTS,                    # ë ˆì´ì–´ë³„ ì´ì›ƒ ìˆ˜\n",
    "    batch_size=BATCH_SIZE,\n",
    "    shuffle=True,\n",
    "    num_workers=NUM_WORKERS,\n",
    "    persistent_workers=PERSISTENT\n",
    ")\n",
    "eval_loader = NeighborLoader(\n",
    "    data_full,\n",
    "    input_nodes=torch.arange(num_nodes),\n",
    "    num_neighbors=FANOUTS,\n",
    "    batch_size=BATCH_SIZE,\n",
    "    shuffle=False,\n",
    "    num_workers=NUM_WORKERS,\n",
    "    persistent_workers=False\n",
    ")\n",
    "\n",
    "# ----------------------- 4) ëª¨ë¸ ì •ì˜ ------------------------------\n",
    "class GAT_GAE(nn.Module):\n",
    "    def __init__(self, in_dim=64, dropout=0.3):\n",
    "        super().__init__()\n",
    "        self.do = dropout\n",
    "        self.g1 = GATConv(in_dim, 32, heads=8, concat=True, dropout=dropout)  # 64 -> 256\n",
    "        self.g2 = GATConv(256, 32, heads=1, concat=True, dropout=dropout)     # 256 -> 32 (z)\n",
    "        self.g3 = GATConv(32, 64, heads=1, concat=True, dropout=dropout)      # 32 -> 64 (x_hat)\n",
    "    def forward(self, x, ei):\n",
    "        x = F.dropout(x, p=self.do, training=self.training); x = F.elu(self.g1(x, ei))\n",
    "        x = F.dropout(x, p=self.do, training=self.training); z = F.elu(self.g2(x, ei))\n",
    "        x_hat = F.dropout(z, p=self.do, training=self.training); x_hat = self.g3(x_hat, ei)\n",
    "        return x_hat, z\n",
    "\n",
    "class SAGE_GAE(nn.Module):\n",
    "    def __init__(self, in_dim=64, dropout=0.3, aggr=\"mean\"):\n",
    "        super().__init__()\n",
    "        self.do = dropout\n",
    "        self.s1 = SAGEConv(in_dim, 256, aggr=aggr)\n",
    "        self.s2 = SAGEConv(256, 32, aggr=aggr)\n",
    "        self.s3 = SAGEConv(32, 64, aggr=aggr)\n",
    "    def forward(self, x, ei):\n",
    "        x = F.dropout(x, p=self.do, training=self.training); x = F.elu(self.s1(x, ei))\n",
    "        z = F.dropout(x, p=self.do, training=self.training); z = F.elu(self.s2(z, ei))\n",
    "        x_hat = F.dropout(z, p=self.do, training=self.training); x_hat = self.s3(x_hat, ei)\n",
    "        return x_hat, z\n",
    "\n",
    "def local_link_bce(z_local, edge_index_local, batch_size):\n",
    "    \"\"\"\n",
    "    z_local: (N_batch_all, d)   # ë¯¸ë‹ˆë°°ì¹˜ ì„œë¸Œê·¸ë˜í”„ì˜ ëª¨ë“  ë…¸ë“œ ì„ë² ë”©\n",
    "    edge_index_local: [2, E_local]  # ë¡œì»¬ ì¸ë±ìŠ¤(0..N_batch_all-1)\n",
    "    batch_size: seed ë…¸ë“œ ê°œìˆ˜ (0..batch_size-1 ê°€ seed)\n",
    "    -> seed ë‚´ë¶€(edge ì–‘ ëì´ seedì— ëª¨ë‘ ì†í•˜ëŠ”) ì–‘ì„± ê°„ì„ ë§Œ ì‚¬ìš©\n",
    "    \"\"\"\n",
    "    src, dst = edge_index_local\n",
    "    seed_mask = (src < batch_size) & (dst < batch_size)\n",
    "    pos = edge_index_local[:, seed_mask]\n",
    "    if pos.numel() == 0:\n",
    "        # seed ë‚´ë¶€ ê°„ì„ ì´ ì—†ìœ¼ë©´ seed ë²”ìœ„ì—ì„œ ì„ì˜ ìŒì„±ë§Œ ì‚¬ìš© (ì–‘ì„± 0 â†’ BCEë§Œìœ¼ë¡  í•™ìŠµ ì˜ë¯¸ ì ìŒ)\n",
    "        # ì•ˆì „í•˜ê²Œ ì•„ì£¼ ì‘ì€ ê°’ ë°˜í™˜\n",
    "        return torch.tensor(0.0, device=z_local.device)\n",
    "    # ë¡œì»¬ seed ì„œë¸Œê·¸ë˜í”„ ê¸°ì¤€ìœ¼ë¡œ negative_sampling\n",
    "    num_neg = pos.size(1)  # pos ìˆ˜ë§Œí¼\n",
    "    neg = negative_sampling(pos, num_nodes=batch_size, num_neg_samples=num_neg, method='sparse')\n",
    "    # ë””ì½”ë”(ì ê³±)\n",
    "    z_seed = z_local[:batch_size]\n",
    "    def dot_decode(z, e): return (z[e[0]] * z[e[1]]).sum(dim=1)\n",
    "    logits = torch.cat([dot_decode(z_seed, pos), dot_decode(z_seed, neg)], dim=0)\n",
    "    labels = torch.cat([torch.ones(pos.size(1), device=logits.device),\n",
    "                        torch.zeros(neg.size(1), device=logits.device)], dim=0)\n",
    "    return F.binary_cross_entropy_with_logits(logits, labels)\n",
    "\n",
    "# ----------------------- 5) ë¯¸ë‹ˆë°°ì¹˜ í•™ìŠµ ë£¨í‹´ ----------------------\n",
    "def train_minibatch(model, epochs, name):\n",
    "    model = model.to(DEVICE)\n",
    "    opt = torch.optim.Adam(model.parameters(), lr=LR, weight_decay=WEIGHT_DECAY)\n",
    "\n",
    "    for ep in range(1, epochs+1):\n",
    "        model.train()\n",
    "        loss_running = mse_running = bce_running = 0.0\n",
    "        steps = 0\n",
    "\n",
    "        for batch in train_loader:\n",
    "            # batch: ë¡œì»¬ ì„œë¸Œê·¸ë˜í”„\n",
    "            # batch.n_id: ê¸€ë¡œë²Œ ë…¸ë“œ ì¸ë±ìŠ¤ (ê¸¸ì´ = N_batch_all)\n",
    "            # batch.batch_size: seed ë…¸ë“œ ê°œìˆ˜\n",
    "            n_id = batch.n_id.to(DEVICE)               # ê¸€ë¡œë²Œ â†’ ì¥ì¹˜\n",
    "            ei   = batch.edge_index.to(DEVICE)         # ë¡œì»¬ ì—£ì§€\n",
    "            x_mb = x_all[n_id]                         # ë¡œì»¬ ë…¸ë“œ íŠ¹ì§•(ìˆœì„œ = ë¡œì»¬ ì¸ë±ìŠ¤)\n",
    "\n",
    "            opt.zero_grad()\n",
    "            x_hat, z = model(x_mb, ei)\n",
    "\n",
    "            # MSE: seed ë…¸ë“œë§Œ\n",
    "            bs = batch.batch_size\n",
    "            mse = F.mse_loss(x_hat[:bs], x_mb[:bs])\n",
    "\n",
    "            # ë§í¬ BCE: seed ë‚´ë¶€ ê°„ì„ ë§Œ\n",
    "            bce = local_link_bce(z, ei, bs)\n",
    "\n",
    "            loss = mse + LAMBDA_BCE * bce\n",
    "            loss.backward(); opt.step()\n",
    "\n",
    "            loss_running += float(loss.item())\n",
    "            mse_running  += float(mse.item())\n",
    "            bce_running  += float(bce.item())\n",
    "            steps += 1\n",
    "\n",
    "        if ep == 1 or ep % 50 == 0:\n",
    "            print(f\"[{name}] {ep:04d} | loss {loss_running/steps:.6f} | mse {mse_running/steps:.6f} | bce {bce_running/steps:.6f}\")\n",
    "\n",
    "    # ------------- í‰ê°€(ë¯¸ë‹ˆë°°ì¹˜ë¡œ ì „ì²´ seed ì¬êµ¬ì„± ë¶™ì´ê¸°) -------------\n",
    "    model.eval()\n",
    "    recon_err = np.zeros(num_nodes, dtype=np.float32)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for batch in eval_loader:\n",
    "            n_id = batch.n_id.to(DEVICE)\n",
    "            ei   = batch.edge_index.to(DEVICE)\n",
    "            x_mb = x_all[n_id]\n",
    "\n",
    "            x_hat, _z = model(x_mb, ei)\n",
    "            bs = batch.batch_size\n",
    "            # seed ëŒ€ì‘ ê¸€ë¡œë²Œ ì¸ë±ìŠ¤\n",
    "            seeds_global = n_id[:bs]\n",
    "            # per-node MSE\n",
    "            err = torch.mean((x_hat[:bs] - x_mb[:bs])**2, dim=1).detach().cpu().numpy()\n",
    "            recon_err[seeds_global.cpu().numpy()] = err\n",
    "\n",
    "    mu, sd = recon_err.mean(), recon_err.std(ddof=1) if recon_err.size > 1 else 1e-8\n",
    "    zscore = (recon_err - mu) / (sd if sd > 0 else 1e-8)\n",
    "    cut = np.percentile(zscore, 95.0)\n",
    "    anom = (zscore > cut).astype(int)\n",
    "\n",
    "    out = pd.DataFrame({\"address\": addresses, \"recon_mse\": recon_err, \"z_score\": zscore, \"is_anomaly_top5pct\": anom})\n",
    "    csv_name = f\"{name.lower()}_mb_anomalies.csv\"\n",
    "    pt_name  = f\"{name.lower()}_mb_model.pt\"\n",
    "    out.sort_values(\"z_score\", ascending=False).to_csv(csv_name, index=False, encoding=\"utf-8\")\n",
    "    torch.save(model.state_dict(), pt_name)\n",
    "    print(f\"âœ… {name} (mini-batch): ì €ì¥ ì™„ë£Œ â†’ {csv_name}, {pt_name}\")\n",
    "    return out\n",
    "\n",
    "# -------------------- 6) ë‘ ëª¨ë¸ ì—°ì† ì‹¤í–‰ ------------------------\n",
    "gat_out  = train_minibatch(GAT_GAE(in_dim=64, dropout=DROPOUT),  EPOCHS, \"GAT_GAE\")\n",
    "sage_out = train_minibatch(SAGE_GAE(in_dim=64, dropout=DROPOUT), EPOCHS, \"SAGE_GAE\")\n",
    "\n",
    "print(\"âœ… ì™„ë£Œ: combined_embeddings_64.npy / scaler.joblib / pca64.joblib ì €ì¥\")\n",
    "print(\"â„¹ï¸ GAT_GAE â†’ gat_gae_mb_anomalies.csv, SAGE_GAE â†’ sage_gae_mb_anomalies.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ec1bec78",
   "metadata": {},
   "outputs": [],
   "source": [
    "gat_ae_an = pd.read_csv(\"gat_ae_anomalies.csv\")\n",
    "sage_ae_an = pd.read_csv(\"sage_ae_anomalies.csv\")\n",
    "\n",
    "gat_gae_an = pd.read_csv(\"gat_gae_anomalies.csv\")\n",
    "sage_gae_an = pd.read_csv(\"sage_gae_anomalies.csv\")\n",
    "\n",
    "gat_gae_mb_an = pd.read_csv(\"gat_gae_mb_anomalies.csv\")\n",
    "sage_gae_mb_an = pd.read_csv(\"sage_gae_mb_anomalies.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "16d8524b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>address</th>\n",
       "      <th>recon_mse</th>\n",
       "      <th>z_score</th>\n",
       "      <th>is_anomaly_top5pct</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0x1938a448d105d26c40a52a1bfe99b8ca7a745ad0</td>\n",
       "      <td>1.074462</td>\n",
       "      <td>16.639975</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0x9a9eb7e103230d3baf2bd2ddc7eae69dbb3f77b8</td>\n",
       "      <td>0.979735</td>\n",
       "      <td>15.079760</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0x167a9333bf582556f35bd4d16a7e80e191aa6476</td>\n",
       "      <td>0.973421</td>\n",
       "      <td>14.975760</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0xf37b1a35647e4efc1afec5bb870e0bcbf1ac2ffc</td>\n",
       "      <td>0.911353</td>\n",
       "      <td>13.953463</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0xf204a7552bb25302a70f8695c7d5edbc8e32cb85</td>\n",
       "      <td>0.897239</td>\n",
       "      <td>13.720983</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0xb5f756611eddfbd63f4e8d28f2a62a401431c35a</td>\n",
       "      <td>0.871787</td>\n",
       "      <td>13.301768</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>0x11784e0732270b41dd7aba1baa266f076b78f085</td>\n",
       "      <td>0.844250</td>\n",
       "      <td>12.848221</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>0xaf0ae50cd011e741cdb90f624b5ff0f06fd6ef58</td>\n",
       "      <td>0.839779</td>\n",
       "      <td>12.774575</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>0x1d5a1eaf90218e91f2bb32e42b0b02ff39827d16</td>\n",
       "      <td>0.829123</td>\n",
       "      <td>12.599068</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>0x19095a519eccd68213b6aa7a80577337d291006e</td>\n",
       "      <td>0.784288</td>\n",
       "      <td>11.860599</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                      address  recon_mse    z_score  \\\n",
       "0  0x1938a448d105d26c40a52a1bfe99b8ca7a745ad0   1.074462  16.639975   \n",
       "1  0x9a9eb7e103230d3baf2bd2ddc7eae69dbb3f77b8   0.979735  15.079760   \n",
       "2  0x167a9333bf582556f35bd4d16a7e80e191aa6476   0.973421  14.975760   \n",
       "3  0xf37b1a35647e4efc1afec5bb870e0bcbf1ac2ffc   0.911353  13.953463   \n",
       "4  0xf204a7552bb25302a70f8695c7d5edbc8e32cb85   0.897239  13.720983   \n",
       "5  0xb5f756611eddfbd63f4e8d28f2a62a401431c35a   0.871787  13.301768   \n",
       "6  0x11784e0732270b41dd7aba1baa266f076b78f085   0.844250  12.848221   \n",
       "7  0xaf0ae50cd011e741cdb90f624b5ff0f06fd6ef58   0.839779  12.774575   \n",
       "8  0x1d5a1eaf90218e91f2bb32e42b0b02ff39827d16   0.829123  12.599068   \n",
       "9  0x19095a519eccd68213b6aa7a80577337d291006e   0.784288  11.860599   \n",
       "\n",
       "   is_anomaly_top5pct  \n",
       "0                   1  \n",
       "1                   1  \n",
       "2                   1  \n",
       "3                   1  \n",
       "4                   1  \n",
       "5                   1  \n",
       "6                   1  \n",
       "7                   1  \n",
       "8                   1  \n",
       "9                   1  "
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gat_ae_an[gat_ae_an['is_anomaly_top5pct'] == 1].head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "417be66d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>address</th>\n",
       "      <th>recon_mse</th>\n",
       "      <th>z_score</th>\n",
       "      <th>is_anomaly_top5pct</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0x9a9eb7e103230d3baf2bd2ddc7eae69dbb3f77b8</td>\n",
       "      <td>0.838493</td>\n",
       "      <td>14.989171</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0x167a9333bf582556f35bd4d16a7e80e191aa6476</td>\n",
       "      <td>0.800249</td>\n",
       "      <td>14.254773</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0x1938a448d105d26c40a52a1bfe99b8ca7a745ad0</td>\n",
       "      <td>0.768789</td>\n",
       "      <td>13.650672</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0xf37b1a35647e4efc1afec5bb870e0bcbf1ac2ffc</td>\n",
       "      <td>0.764231</td>\n",
       "      <td>13.563149</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0xf204a7552bb25302a70f8695c7d5edbc8e32cb85</td>\n",
       "      <td>0.760840</td>\n",
       "      <td>13.498028</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0xb5f756611eddfbd63f4e8d28f2a62a401431c35a</td>\n",
       "      <td>0.728500</td>\n",
       "      <td>12.877007</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>0x1d5a1eaf90218e91f2bb32e42b0b02ff39827d16</td>\n",
       "      <td>0.727814</td>\n",
       "      <td>12.863837</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>0x11784e0732270b41dd7aba1baa266f076b78f085</td>\n",
       "      <td>0.703027</td>\n",
       "      <td>12.387865</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>0xaf0ae50cd011e741cdb90f624b5ff0f06fd6ef58</td>\n",
       "      <td>0.683921</td>\n",
       "      <td>12.020988</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>0x19095a519eccd68213b6aa7a80577337d291006e</td>\n",
       "      <td>0.681271</td>\n",
       "      <td>11.970102</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                      address  recon_mse    z_score  \\\n",
       "0  0x9a9eb7e103230d3baf2bd2ddc7eae69dbb3f77b8   0.838493  14.989171   \n",
       "1  0x167a9333bf582556f35bd4d16a7e80e191aa6476   0.800249  14.254773   \n",
       "2  0x1938a448d105d26c40a52a1bfe99b8ca7a745ad0   0.768789  13.650672   \n",
       "3  0xf37b1a35647e4efc1afec5bb870e0bcbf1ac2ffc   0.764231  13.563149   \n",
       "4  0xf204a7552bb25302a70f8695c7d5edbc8e32cb85   0.760840  13.498028   \n",
       "5  0xb5f756611eddfbd63f4e8d28f2a62a401431c35a   0.728500  12.877007   \n",
       "6  0x1d5a1eaf90218e91f2bb32e42b0b02ff39827d16   0.727814  12.863837   \n",
       "7  0x11784e0732270b41dd7aba1baa266f076b78f085   0.703027  12.387865   \n",
       "8  0xaf0ae50cd011e741cdb90f624b5ff0f06fd6ef58   0.683921  12.020988   \n",
       "9  0x19095a519eccd68213b6aa7a80577337d291006e   0.681271  11.970102   \n",
       "\n",
       "   is_anomaly_top5pct  \n",
       "0                   1  \n",
       "1                   1  \n",
       "2                   1  \n",
       "3                   1  \n",
       "4                   1  \n",
       "5                   1  \n",
       "6                   1  \n",
       "7                   1  \n",
       "8                   1  \n",
       "9                   1  "
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sage_ae_an[sage_ae_an['is_anomaly_top5pct'] == 1].head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "27fb1aec",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>address</th>\n",
       "      <th>recon_mse</th>\n",
       "      <th>z_score</th>\n",
       "      <th>is_anomaly_top5pct</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0x167a9333bf582556f35bd4d16a7e80e191aa6476</td>\n",
       "      <td>982.400940</td>\n",
       "      <td>73.656010</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0x0ce15800ebc76a1034d320ad2abcd0e77ba52ece</td>\n",
       "      <td>295.798220</td>\n",
       "      <td>22.151089</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0x1f69b6147203344049ea381f5dd2008714caedda</td>\n",
       "      <td>280.451140</td>\n",
       "      <td>20.999840</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0xd28493e737fbcc957f3716143ed6e40f40357b51</td>\n",
       "      <td>275.396060</td>\n",
       "      <td>20.620638</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0xd4b394c60bb55f80df30dac87b6f92be34739332</td>\n",
       "      <td>254.439870</td>\n",
       "      <td>19.048626</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0x0000000000000000000000000000000000000000</td>\n",
       "      <td>232.739840</td>\n",
       "      <td>17.420818</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>0x1938a448d105d26c40a52a1bfe99b8ca7a745ad0</td>\n",
       "      <td>205.130690</td>\n",
       "      <td>15.349741</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>0xa5025faba6e70b84f74e9b1113e5f7f4e7f4859f</td>\n",
       "      <td>123.233376</td>\n",
       "      <td>9.206282</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>0xfea2c6d96f0bd4cd4d638bcefa7968146c74df37</td>\n",
       "      <td>110.310020</td>\n",
       "      <td>8.236848</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>0x1d5a1eaf90218e91f2bb32e42b0b02ff39827d16</td>\n",
       "      <td>78.658590</td>\n",
       "      <td>5.862543</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                      address   recon_mse    z_score  \\\n",
       "0  0x167a9333bf582556f35bd4d16a7e80e191aa6476  982.400940  73.656010   \n",
       "1  0x0ce15800ebc76a1034d320ad2abcd0e77ba52ece  295.798220  22.151089   \n",
       "2  0x1f69b6147203344049ea381f5dd2008714caedda  280.451140  20.999840   \n",
       "3  0xd28493e737fbcc957f3716143ed6e40f40357b51  275.396060  20.620638   \n",
       "4  0xd4b394c60bb55f80df30dac87b6f92be34739332  254.439870  19.048626   \n",
       "5  0x0000000000000000000000000000000000000000  232.739840  17.420818   \n",
       "6  0x1938a448d105d26c40a52a1bfe99b8ca7a745ad0  205.130690  15.349741   \n",
       "7  0xa5025faba6e70b84f74e9b1113e5f7f4e7f4859f  123.233376   9.206282   \n",
       "8  0xfea2c6d96f0bd4cd4d638bcefa7968146c74df37  110.310020   8.236848   \n",
       "9  0x1d5a1eaf90218e91f2bb32e42b0b02ff39827d16   78.658590   5.862543   \n",
       "\n",
       "   is_anomaly_top5pct  \n",
       "0                   1  \n",
       "1                   1  \n",
       "2                   1  \n",
       "3                   1  \n",
       "4                   1  \n",
       "5                   1  \n",
       "6                   1  \n",
       "7                   1  \n",
       "8                   1  \n",
       "9                   1  "
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gat_gae_an[gat_gae_an['is_anomaly_top5pct'] == 1].head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "52e6ebfa",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>address</th>\n",
       "      <th>recon_mse</th>\n",
       "      <th>z_score</th>\n",
       "      <th>is_anomaly_top5pct</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0x167a9333bf582556f35bd4d16a7e80e191aa6476</td>\n",
       "      <td>980.456400</td>\n",
       "      <td>73.610580</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0x0ce15800ebc76a1034d320ad2abcd0e77ba52ece</td>\n",
       "      <td>293.959170</td>\n",
       "      <td>22.043434</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0x1f69b6147203344049ea381f5dd2008714caedda</td>\n",
       "      <td>279.166100</td>\n",
       "      <td>20.932234</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0xd28493e737fbcc957f3716143ed6e40f40357b51</td>\n",
       "      <td>273.299320</td>\n",
       "      <td>20.491543</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0xd4b394c60bb55f80df30dac87b6f92be34739332</td>\n",
       "      <td>250.081900</td>\n",
       "      <td>18.747536</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0x0000000000000000000000000000000000000000</td>\n",
       "      <td>246.659940</td>\n",
       "      <td>18.490492</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>0x1938a448d105d26c40a52a1bfe99b8ca7a745ad0</td>\n",
       "      <td>204.375640</td>\n",
       "      <td>15.314251</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>0xa5025faba6e70b84f74e9b1113e5f7f4e7f4859f</td>\n",
       "      <td>120.477280</td>\n",
       "      <td>9.012116</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>0xfea2c6d96f0bd4cd4d638bcefa7968146c74df37</td>\n",
       "      <td>110.667700</td>\n",
       "      <td>8.275256</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>0x1d5a1eaf90218e91f2bb32e42b0b02ff39827d16</td>\n",
       "      <td>77.889534</td>\n",
       "      <td>5.813081</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                      address   recon_mse    z_score  \\\n",
       "0  0x167a9333bf582556f35bd4d16a7e80e191aa6476  980.456400  73.610580   \n",
       "1  0x0ce15800ebc76a1034d320ad2abcd0e77ba52ece  293.959170  22.043434   \n",
       "2  0x1f69b6147203344049ea381f5dd2008714caedda  279.166100  20.932234   \n",
       "3  0xd28493e737fbcc957f3716143ed6e40f40357b51  273.299320  20.491543   \n",
       "4  0xd4b394c60bb55f80df30dac87b6f92be34739332  250.081900  18.747536   \n",
       "5  0x0000000000000000000000000000000000000000  246.659940  18.490492   \n",
       "6  0x1938a448d105d26c40a52a1bfe99b8ca7a745ad0  204.375640  15.314251   \n",
       "7  0xa5025faba6e70b84f74e9b1113e5f7f4e7f4859f  120.477280   9.012116   \n",
       "8  0xfea2c6d96f0bd4cd4d638bcefa7968146c74df37  110.667700   8.275256   \n",
       "9  0x1d5a1eaf90218e91f2bb32e42b0b02ff39827d16   77.889534   5.813081   \n",
       "\n",
       "   is_anomaly_top5pct  \n",
       "0                   1  \n",
       "1                   1  \n",
       "2                   1  \n",
       "3                   1  \n",
       "4                   1  \n",
       "5                   1  \n",
       "6                   1  \n",
       "7                   1  \n",
       "8                   1  \n",
       "9                   1  "
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sage_gae_an[sage_gae_an['is_anomaly_top5pct'] == 1].head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "bc656577",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>address</th>\n",
       "      <th>recon_mse</th>\n",
       "      <th>z_score</th>\n",
       "      <th>is_anomaly_top5pct</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0x167a9333bf582556f35bd4d16a7e80e191aa6476</td>\n",
       "      <td>982.01010</td>\n",
       "      <td>74.976320</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0x0ce15800ebc76a1034d320ad2abcd0e77ba52ece</td>\n",
       "      <td>294.19693</td>\n",
       "      <td>22.437508</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0x1f69b6147203344049ea381f5dd2008714caedda</td>\n",
       "      <td>279.11370</td>\n",
       "      <td>21.285372</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0xd4b394c60bb55f80df30dac87b6f92be34739332</td>\n",
       "      <td>254.99422</td>\n",
       "      <td>19.442997</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0xd28493e737fbcc957f3716143ed6e40f40357b51</td>\n",
       "      <td>213.19151</td>\n",
       "      <td>16.249886</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0x1938a448d105d26c40a52a1bfe99b8ca7a745ad0</td>\n",
       "      <td>204.81764</td>\n",
       "      <td>15.610245</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>0x0000000000000000000000000000000000000000</td>\n",
       "      <td>191.34544</td>\n",
       "      <td>14.581166</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>0xa5025faba6e70b84f74e9b1113e5f7f4e7f4859f</td>\n",
       "      <td>121.95533</td>\n",
       "      <td>9.280783</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>0xfea2c6d96f0bd4cd4d638bcefa7968146c74df37</td>\n",
       "      <td>111.11345</td>\n",
       "      <td>8.452622</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>0x1d5a1eaf90218e91f2bb32e42b0b02ff39827d16</td>\n",
       "      <td>78.07902</td>\n",
       "      <td>5.929277</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                      address  recon_mse    z_score  \\\n",
       "0  0x167a9333bf582556f35bd4d16a7e80e191aa6476  982.01010  74.976320   \n",
       "1  0x0ce15800ebc76a1034d320ad2abcd0e77ba52ece  294.19693  22.437508   \n",
       "2  0x1f69b6147203344049ea381f5dd2008714caedda  279.11370  21.285372   \n",
       "3  0xd4b394c60bb55f80df30dac87b6f92be34739332  254.99422  19.442997   \n",
       "4  0xd28493e737fbcc957f3716143ed6e40f40357b51  213.19151  16.249886   \n",
       "5  0x1938a448d105d26c40a52a1bfe99b8ca7a745ad0  204.81764  15.610245   \n",
       "6  0x0000000000000000000000000000000000000000  191.34544  14.581166   \n",
       "7  0xa5025faba6e70b84f74e9b1113e5f7f4e7f4859f  121.95533   9.280783   \n",
       "8  0xfea2c6d96f0bd4cd4d638bcefa7968146c74df37  111.11345   8.452622   \n",
       "9  0x1d5a1eaf90218e91f2bb32e42b0b02ff39827d16   78.07902   5.929277   \n",
       "\n",
       "   is_anomaly_top5pct  \n",
       "0                   1  \n",
       "1                   1  \n",
       "2                   1  \n",
       "3                   1  \n",
       "4                   1  \n",
       "5                   1  \n",
       "6                   1  \n",
       "7                   1  \n",
       "8                   1  \n",
       "9                   1  "
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gat_gae_mb_an[gat_gae_mb_an['is_anomaly_top5pct'] == 1].head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "56e3c009",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>address</th>\n",
       "      <th>recon_mse</th>\n",
       "      <th>z_score</th>\n",
       "      <th>is_anomaly_top5pct</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0x167a9333bf582556f35bd4d16a7e80e191aa6476</td>\n",
       "      <td>970.829200</td>\n",
       "      <td>79.920140</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0x1f69b6147203344049ea381f5dd2008714caedda</td>\n",
       "      <td>278.117030</td>\n",
       "      <td>22.873000</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0x0ce15800ebc76a1034d320ad2abcd0e77ba52ece</td>\n",
       "      <td>271.768980</td>\n",
       "      <td>22.350216</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0x1938a448d105d26c40a52a1bfe99b8ca7a745ad0</td>\n",
       "      <td>183.511540</td>\n",
       "      <td>15.081924</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0xd28493e737fbcc957f3716143ed6e40f40357b51</td>\n",
       "      <td>119.247780</td>\n",
       "      <td>9.789589</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0xfea2c6d96f0bd4cd4d638bcefa7968146c74df37</td>\n",
       "      <td>91.081800</td>\n",
       "      <td>7.470028</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>0x0000000000000000000000000000000000000000</td>\n",
       "      <td>81.365660</td>\n",
       "      <td>6.669871</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>0x1d5a1eaf90218e91f2bb32e42b0b02ff39827d16</td>\n",
       "      <td>74.387160</td>\n",
       "      <td>6.095169</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>0xd4b394c60bb55f80df30dac87b6f92be34739332</td>\n",
       "      <td>64.854290</td>\n",
       "      <td>5.310105</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>0xf204a7552bb25302a70f8695c7d5edbc8e32cb85</td>\n",
       "      <td>58.322456</td>\n",
       "      <td>4.772187</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                      address   recon_mse    z_score  \\\n",
       "0  0x167a9333bf582556f35bd4d16a7e80e191aa6476  970.829200  79.920140   \n",
       "1  0x1f69b6147203344049ea381f5dd2008714caedda  278.117030  22.873000   \n",
       "2  0x0ce15800ebc76a1034d320ad2abcd0e77ba52ece  271.768980  22.350216   \n",
       "3  0x1938a448d105d26c40a52a1bfe99b8ca7a745ad0  183.511540  15.081924   \n",
       "4  0xd28493e737fbcc957f3716143ed6e40f40357b51  119.247780   9.789589   \n",
       "5  0xfea2c6d96f0bd4cd4d638bcefa7968146c74df37   91.081800   7.470028   \n",
       "6  0x0000000000000000000000000000000000000000   81.365660   6.669871   \n",
       "7  0x1d5a1eaf90218e91f2bb32e42b0b02ff39827d16   74.387160   6.095169   \n",
       "8  0xd4b394c60bb55f80df30dac87b6f92be34739332   64.854290   5.310105   \n",
       "9  0xf204a7552bb25302a70f8695c7d5edbc8e32cb85   58.322456   4.772187   \n",
       "\n",
       "   is_anomaly_top5pct  \n",
       "0                   1  \n",
       "1                   1  \n",
       "2                   1  \n",
       "3                   1  \n",
       "4                   1  \n",
       "5                   1  \n",
       "6                   1  \n",
       "7                   1  \n",
       "8                   1  \n",
       "9                   1  "
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sage_gae_mb_an[sage_gae_mb_an['is_anomaly_top5pct'] == 1].head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9249e360",
   "metadata": {},
   "outputs": [],
   "source": [
    "gat_ae_anomalies, sage_ae_anomalies, gat_gae_anomalies, sage_gae_anomalies, gat_gae_mb_anomalies, sage_gae_mb_anomalies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "f7757e95",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== ë¹„êµ ê²°ê³¼: gat_ae_anomalies.csv  vs  sage_ae_anomalies.csv ===\n",
      "A(=ì™¼ìª½) ì´ìƒì¹˜ ìˆ˜: 398\n",
      "B(=ì˜¤ë¥¸ìª½) ì´ìƒì¹˜ ìˆ˜: 398\n",
      "êµì§‘í•©: 385 / í•©ì§‘í•©: 411\n",
      "Jaccard: 0.9367\n",
      "Overlap Coefficient: 0.9673  (|Aâˆ©B| / min(|A|,|B|))\n",
      "Precision(Aâ†’B): 0.9673   Recall(Aâ†’B): 0.9673   F1: 0.9673\n",
      "íŒŒì¼ ì €ì¥ â†’ common_gat_ae_anomalies__sage_ae_anomalies.csv, only_gat_ae_anomalies.csv, only_sage_ae_anomalies.csv\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from pathlib import Path\n",
    "\n",
    "# ============================================\n",
    "# íŒŒì¼ëª…ë§Œ ë°”ê¿”ì„œ ì‹¤í–‰í•˜ì„¸ìš” (ì˜ˆì‹œ)\n",
    "file_a = \"gat_ae_anomalies\"   # ì˜ˆ: \"gat_ae_an\", \"sage_gae_an\", ...\n",
    "file_b = \"sage_ae_anomalies\"\n",
    "# ============================================\n",
    "\n",
    "def ensure_csv(path_like: str) -> str:\n",
    "    return path_like if str(path_like).lower().endswith(\".csv\") else str(path_like) + \".csv\"\n",
    "\n",
    "def load_anomaly_addresses(csv_path: str) -> pd.Series:\n",
    "    df = pd.read_csv(csv_path)\n",
    "    # ì»¬ëŸ¼ ì²´í¬\n",
    "    required = {\"address\", \"is_anomaly_top5pct\"}\n",
    "    if not required.issubset(df.columns):\n",
    "        missing = required - set(df.columns)\n",
    "        raise ValueError(f\"{csv_path}: í•„ìš”í•œ ì»¬ëŸ¼ì´ ì—†ìŠµë‹ˆë‹¤ â†’ {missing}\")\n",
    "    # í•„í„°ë§: is_anomaly_top5pct == 1\n",
    "    s = df.loc[(df[\"is_anomaly_top5pct\"].astype(float) == 1), \"address\"]\n",
    "    # ì£¼ì†Œ ì •ê·œí™”(ì†Œë¬¸ì/ê³µë°± ì œê±°)\n",
    "    s = s.astype(str).str.strip().str.lower()\n",
    "    return s.dropna()\n",
    "\n",
    "def compare_two(csv_a: str, csv_b: str):\n",
    "    csv_a = ensure_csv(csv_a)\n",
    "    csv_b = ensure_csv(csv_b)\n",
    "\n",
    "    a = load_anomaly_addresses(csv_a)\n",
    "    b = load_anomaly_addresses(csv_b)\n",
    "\n",
    "    A = set(a.unique())\n",
    "    B = set(b.unique())\n",
    "    inter = A & B\n",
    "    union = A | B\n",
    "\n",
    "    # ì§€í‘œë“¤\n",
    "    nA, nB, nI, nU = len(A), len(B), len(inter), len(union)\n",
    "    jaccard = nI / nU if nU else 0.0\n",
    "    overlap_coeff = nI / min(nA, nB) if min(nA, nB) else 0.0\n",
    "    prec_A_to_B = nI / nA if nA else 0.0\n",
    "    rec_A_to_B  = nI / nB if nB else 0.0\n",
    "    f1 = (2*prec_A_to_B*rec_A_to_B)/(prec_A_to_B+rec_A_to_B) if (prec_A_to_B+rec_A_to_B)>0 else 0.0\n",
    "\n",
    "    print(f\"=== ë¹„êµ ê²°ê³¼: {Path(csv_a).name}  vs  {Path(csv_b).name} ===\")\n",
    "    print(f\"A(=ì™¼ìª½) ì´ìƒì¹˜ ìˆ˜: {nA}\")\n",
    "    print(f\"B(=ì˜¤ë¥¸ìª½) ì´ìƒì¹˜ ìˆ˜: {nB}\")\n",
    "    print(f\"êµì§‘í•©: {nI} / í•©ì§‘í•©: {nU}\")\n",
    "    print(f\"Jaccard: {jaccard:.4f}\")\n",
    "    print(f\"Overlap Coefficient: {overlap_coeff:.4f}  (|Aâˆ©B| / min(|A|,|B|))\")\n",
    "    print(f\"Precision(Aâ†’B): {prec_A_to_B:.4f}   Recall(Aâ†’B): {rec_A_to_B:.4f}   F1: {f1:.4f}\")\n",
    "\n",
    "    # ê²°ê³¼ë¥¼ íŒŒì¼ë¡œë„ ì €ì¥(ì›í•˜ë©´ ì£¼ì„ í•´ì œ)\n",
    "    stemA, stemB = Path(csv_a).stem, Path(csv_b).stem\n",
    "    pd.Series(sorted(inter), name=\"address\").to_csv(f\"common_{stemA}__{stemB}.csv\", index=False)\n",
    "    pd.Series(sorted(A - B), name=\"address\").to_csv(f\"only_{stemA}.csv\", index=False)\n",
    "    pd.Series(sorted(B - A), name=\"address\").to_csv(f\"only_{stemB}.csv\", index=False)\n",
    "    print(f\"íŒŒì¼ ì €ì¥ â†’ common_{stemA}__{stemB}.csv, only_{stemA}.csv, only_{stemB}.csv\")\n",
    "\n",
    "# ì‹¤í–‰\n",
    "compare_two(file_a, file_b)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
